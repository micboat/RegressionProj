<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Model Design</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<script src="site_libs/navigation-1.1/codefolding.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->
<style type="text/css">
.code-folding-btn { margin-bottom: 4px; }
</style>
<script>
$(document).ready(function () {
  window.initializeCodeFolding("show" === "show");
});
</script>




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Machine Learning</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="nobroker-extract-data.html">Data Extract</a>
</li>
<li>
  <a href="Part_II.html">Data Cleaning</a>
</li>
<li>
  <a href="Part_IV_ML.html">Model Design</a>
</li>
<li>
  <a href="Part III.html">Deploy</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">

<div class="btn-group pull-right">
<button type="button" class="btn btn-default btn-xs dropdown-toggle" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false"><span>Code</span> <span class="caret"></span></button>
<ul class="dropdown-menu" style="min-width: 50px;">
<li><a id="rmd-show-all-code" href="#">Show All Code</a></li>
<li><a id="rmd-hide-all-code" href="#">Hide All Code</a></li>
</ul>
</div>



<h1 class="title toc-ignore">Model Design</h1>

</div>


<pre class="r"><code>library(plyr);library(tidyverse); library(caret); library(tidymodels); library(ranger)

library(ranger); library(xgboost); library(glmnet); library(e1071); library(pls); library(Cubist); library(earth); library(Cubist); library(DALEX); library(breakDown)

nb_data &lt;- readRDS(&quot;nb_data.rds&quot;)</code></pre>
<p>In this phase we will be preparing various regression models for predicting house prices.</p>
<div id="which-model-to-select" class="section level2">
<h2>Which model to select?</h2>
<p>Borrowed from Aurélien Géron “Hands on ML” book, we will use the following steps to reach promising models.</p>
<ol style="list-style-type: decimal">
<li>Train many quick and dirty models from different categories using standard parameters.<br />
</li>
<li>Measure and compare their performance.<br />
</li>
<li>Analyze the most significant variables for each algorithm.</li>
<li>Have a quick round of feature selection and engineering.</li>
<li>Short-list the top three to five most promising models, preferring models that make different types of errors.</li>
</ol>
</div>
<div id="split-data" class="section level2">
<h2>Split Data</h2>
<p>Lets split our data in test and training set. We will use the rsample package for split</p>
<p>Note:- nb_data is already loaded in our workspace. It can be downloaded from here.</p>
<pre class="r"><code>set.seed(42)

train_test_split &lt;- initial_split(nb_data)

nb_train &lt;- training(train_test_split)
nb_test &lt;- testing(train_test_split)</code></pre>
</div>
<div id="pre-process" class="section level2">
<h2>Pre Process</h2>
<p>Lets impute missing values. We alrady have the missing value stats with us (from module 1), lets recollect it.</p>
<table>
<thead>
<tr class="header">
<th>Features</th>
<th>Missing values</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>FloorType</td>
<td>209</td>
</tr>
<tr class="even">
<td>Facing</td>
<td>1153</td>
</tr>
<tr class="odd">
<td>PowerBackup</td>
<td>4229</td>
</tr>
<tr class="even">
<td>WaterSupply</td>
<td>116</td>
</tr>
</tbody>
</table>
<p>Since we will run multiple models, first lets create a structure for our model i.e. set preprocessing steps to our model.</p>
<pre class="r"><code>rec_obj &lt;- recipe(HousePriceinK ~ ., data = nb_train) %&gt;% #1
  step_knnimpute(all_predictors()) %&gt;%  #2
  #step_other(NoofBathrooms, NoofBalconies, threshold = 0.05) %&gt;%
  step_dummy(all_predictors(), -all_numeric()) #%&gt;% #3
  #step_log(all_outcomes()) %&gt;% 
  #prep(training = nb_train) #4

#step_bs(Lat, Long)

prepare_rec &lt;- rec_obj %&gt;% prep(training = nb_train)

train_data &lt;- prepare_rec %&gt;% #5
  bake(nb_train) %&gt;% 
  select(-HousePriceinK, everything())

test_data &lt;- prepare_rec %&gt;% #5
  bake(nb_train) %&gt;% 
  select(-HousePriceinK, everything())

x_col &lt;- 1:35
y_col &lt;- 36</code></pre>
<p>Let’s understand what are we doing in above code!</p>
<p>1 - Creating a structure for our model, the data argument is just to identify variables. 2 - Impute missing value through KNN algorithim, the default K is 5. 3 - Convert nominal data into numeric hot encoding. 4 - Run preprocess step on our data 5 - Apply our preprossing steps on training and test set.</p>
<p>We can apply various preprocessing steps such as stp_center, step_scale, etc from the <a href="https://tidymodels.github.io/recipes/index.html">recipes</a> package.</p>
</div>
<div id="train-model" class="section level2">
<h2>Train Model</h2>
<p>We will train the following ten models; the applied predictive modeling book suggest to start from complex and move towards simpler modelling techniques.</p>
<ol style="list-style-type: decimal">
<li>RandomForest</li>
<li>xgBoost</li>
<li>Ridge Regression</li>
<li>Lasso Regression</li>
<li>ElasticNet Regression</li>
<li>Support vector regression</li>
<li>Principal Component Regression (PCR)</li>
<li>Cubist</li>
<li>Multivariate Adaptive Regression Splines (MARS)</li>
<li>Multiple Linear Regression</li>
</ol>
<p>Click here to move to resutls</p>
<div id="random-forest" class="section level4">
<h4>Random Forest</h4>
<p>Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions.</p>
<pre class="r"><code>library(ranger)

rf_model &lt;- ranger(HousePriceinK ~ ., data = train_data, num.trees = 500)

rf_prediction &lt;- rf_model %&gt;% 
  predict(train_data[x_col]) %&gt;% 
  predictions() </code></pre>
</div>
<div id="xgboost" class="section level4">
<h4>xgBoost</h4>
<p>Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor.</p>
<pre class="r"><code>library(xgboost)

xgb_model &lt;- xgboost(data = as.matrix(train_data[x_col]), label = as.matrix(train_data[y_col]), nrounds = 500, verbose = 0)
xg_prediction &lt;- xgb_model %&gt;% 
  predict(as.matrix(train_data[x_col]))</code></pre>
</div>
<div id="ridge-regression" class="section level4">
<h4>Ridge Regression</h4>
<pre class="r"><code>library(glmnet)

ridge_model &lt;- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 0)

ridge_predict&lt;- ridge_model %&gt;% 
  predict(as.matrix(train_data[x_col]), s = ridge_model$lambda.min) #s is panelty value</code></pre>
</div>
<div id="lasso-regression" class="section level4">
<h4>Lasso Regression</h4>
<pre class="r"><code>library(glmnet)

lasso_model &lt;- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 1)

lasso_predict&lt;- lasso_model %&gt;% 
  predict(as.matrix(train_data[x_col]), s = lasso_model$lambda.min)</code></pre>
</div>
<div id="elasticnet-regression" class="section level4">
<h4>ElasticNet Regression</h4>
<pre class="r"><code>elastic_model &lt;- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 0.5)

elastic_predict&lt;- elastic_model %&gt;% 
  predict(as.matrix(train_data[x_col]), s = elastic_model$lambda.min)</code></pre>
</div>
<div id="support-vector-regression" class="section level4">
<h4>Support Vector Regression</h4>
<pre class="r"><code>library(e1071)

svr_model &lt;- svm(HousePriceinK ~ ., data = train_data, type = &quot;eps-regression&quot;, kernel = &quot;radial&quot;)

svr_predict &lt;- svr_model %&gt;% 
  predict(train_data[x_col])</code></pre>
</div>
<div id="principal-component-regression-pcr" class="section level4">
<h4>Principal component regression (PCR)</h4>
<p>The principal components regression (PCR) approach involves constructing principal components , and then using these components as the predictors in a linear regression model that is fit using least squares.</p>
<pre class="r"><code>library(pls)

pcr_model &lt;- pcr(HousePriceinK ~ ., data = train_data, scale = TRUE)

pcr_predict &lt;- pcr_model %&gt;% 
  predict(train_data[x_col], ncomp = 34)

#Using summary(pcr_model), we identify that with 34 ncomp 95% of X is retained.</code></pre>
</div>
<div id="cubist" class="section level4">
<h4>Cubist</h4>
<p>Like xgBoost, Cubist is a boosting techinque plus it also performs neighbour based instance techinque to get result.</p>
<pre class="r"><code>library(Cubist)

cubist_model &lt;- cubist(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), committees = 5)

cubist_predict &lt;- cubist_model %&gt;% 
  predict(as.matrix(train_data[x_col]))</code></pre>
</div>
<div id="multivariate-adaptive-regression-splines" class="section level4">
<h4>Multivariate Adaptive Regression Splines</h4>
<pre class="r"><code>library(earth)

mars_model &lt;- earth(HousePriceinK ~ ., data = train_data)

mars_predict &lt;- mars_model %&gt;% 
  predict(train_data[x_col])</code></pre>
</div>
<div id="linear-regression" class="section level4">
<h4>Linear Regression</h4>
<pre class="r"><code>lm_model &lt;- lm(HousePriceinK ~ ., data = train_data)

lm_predict &lt;- lm_model %&gt;% 
  predict(train_data[x_col])</code></pre>
</div>
</div>
<div id="compare-performance" class="section level2">
<h2>Compare Performance</h2>
<p>For a regression problem, the Root Mean Square Error - RMSE is a best indicator of performance</p>
<pre class="r"><code># Since house price was transformed to log values, we convert it back using exp()
results &lt;- tibble(
  Actual = train_data$HousePriceinK,
  RF = rf_prediction,
  xg = xg_prediction,
  ridge = ridge_predict %&gt;% as.vector(),
  lasso = lasso_predict %&gt;% as.vector(),
  elasticnet = elastic_predict %&gt;% as.vector(),
  svr = svr_predict %&gt;% as.vector(),
  PCR = pcr_predict %&gt;% as.vector(),
  cubist = cubist_predict,
  MARS = mars_predict %&gt;% as.vector(),
  MLR = lm_predict %&gt;% as.vector()
)


RMSE &lt;- tibble(
  RMSE_RF = results %&gt;% rmse(Actual, RF),
  RMSE_XGB = results %&gt;% rmse(Actual, xg),
  RMSE_Ridge = results %&gt;% rmse(Actual, ridge),
  RMSE_Lasso = results %&gt;% rmse(Actual, lasso),
  RMSE_Elastic = results %&gt;% rmse(Actual, elasticnet),
  RMSE_SVR = results %&gt;% rmse(Actual, svr),
  RMSE_PCR = results %&gt;% rmse(Actual, PCR),
  RMSE_Cubist = results %&gt;% rmse(Actual, cubist),
  RMSE_MARS = results %&gt;% rmse(Actual, MARS),
  RMSE_MLR = results %&gt;% rmse(Actual, MLR),
  
) %&gt;% 
  gather(&quot;Model&quot;, 1:10, value = &quot;RMSE&quot;) %&gt;% 
  arrange(RMSE)

RMSE</code></pre>
<pre><code>## # A tibble: 10 x 2
##    Model         RMSE
##    &lt;chr&gt;        &lt;dbl&gt;
##  1 RMSE_XGB      219.
##  2 RMSE_RF      4506.
##  3 RMSE_Cubist  6805.
##  4 RMSE_SVR     8305.
##  5 RMSE_MARS    9203.
##  6 RMSE_MLR     9886.
##  7 RMSE_PCR     9889.
##  8 RMSE_Lasso   9895.
##  9 RMSE_Elastic 9897.
## 10 RMSE_Ridge   9937.</code></pre>
<pre class="r"><code>rm(results, lm_model, mars_model, lm_predict, mars_predict, cubist_predict, cubist_model, pcr_model, pcr_predict, svr_model, svr_predict, elastic_model, elastic_predict, lasso_model, lasso_predict, ridge_model, ridge_predict, xgb_model, xg_prediction, rf_model, rf_prediction, train_test_split)</code></pre>
<p>With default values of the model, we can see that xgBoost turns out to provide the best performance. But these values do not provide a measurement of spread.</p>
<p>Next we will use cross validation technique to evaluate overfitting of models and imporve model performance. Further to that we will check if tuning model hyperparameters could help to increase performance.</p>
</div>
<div id="cross-validation" class="section level2">
<h2>Cross Validation</h2>
<p>Lets create cross validation samples of our data. We will create 10 cross validation set.</p>
<pre class="r"><code>set.seed(42)

#Create 10 splits of data, repeat it for 5 times i.e create 50 samples.
cv_train &lt;- nb_train %&gt;% vfold_cv(v = 10, repeats = 10)

first_sample &lt;- cv_train$splits[[1]]

dim(first_sample)</code></pre>
<pre><code>##   analysis assessment          n          p 
##       4482        499       4981         18</code></pre>
<p>As we see for a single sample we have total 4986 observation, this will create approx 4487 (90%) observation to perform training and approx 499 (10%) observation for testing purpose.</p>
<p>Recipe should be applied on assessment set, so that blanks are imputed accordingly.</p>
<pre class="r"><code>multiple_model &lt;- function(split, ...){

cv_trainset &lt;- prepare_rec %&gt;% bake(analysis(split)) %&gt;% select(-HousePriceinK, everything())
cv_testset &lt;- prepare_rec %&gt;% bake(assessment(split)) %&gt;% select(-HousePriceinK)

rfmod &lt;- ranger(HousePriceinK ~ ., data = cv_trainset, num.trees = 500)
xgmod &lt;- xgboost(data = as.matrix(cv_trainset[x_col]), label = as.matrix(cv_trainset[y_col]), nrounds = 500, verbose = 0)
ridgemod &lt;- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 0)
lassomod &lt;- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 1)
elasticmod &lt;- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 0.5)
svrmod &lt;- svm(HousePriceinK ~ ., data = cv_trainset, type = &quot;eps-regression&quot;, kernel = &quot;radial&quot;)
pcrmodel &lt;- pcr(HousePriceinK ~ ., data = cv_trainset)
cubistmod &lt;- cubist(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), committees = 5)
marsmod &lt;- earth(HousePriceinK ~ ., data = cv_trainset)
lmmod &lt;- lm(HousePriceinK ~ ., data = cv_trainset)

cv_results &lt;- tibble(
  actual = assessment(split) %&gt;% select(HousePriceinK) %&gt;% pull(),
  rfpred = rfmod %&gt;% predict(cv_testset) %&gt;% predictions(),
  xgpred = xgmod %&gt;% predict(as.matrix(cv_testset)),
  ridpred = ridgemod %&gt;% predict(as.matrix(cv_testset), s = ridgemod$lambda.min) %&gt;% as.vector(),
  lassopred = lassomod %&gt;% predict(as.matrix(cv_testset), s = lassomod$lambda.min) %&gt;% as.vector(),
  elasticpred = elasticmod %&gt;% predict(as.matrix(cv_testset), s = elasticmod$lambda.min) %&gt;% as.vector(),
  svrpred = svrmod %&gt;% predict(cv_testset) %&gt;% as.vector(),
  pcrpred = pcrmodel %&gt;% predict(cv_testset, ncomp = 34) %&gt;% as.vector(),
  cubistpred = cubistmod %&gt;% predict(cv_testset),
  marspred = marsmod %&gt;% predict(cv_testset) %&gt;% as.vector(),
  lmpred = lmmod %&gt;% predict(cv_testset) %&gt;% as.vector()

)

cv_RMSE &lt;- tibble(
  randomforest = cv_results %&gt;% rmse(actual, rfpred),
  xgboost = cv_results %&gt;% rmse(actual, xgpred),
  Ridge = cv_results %&gt;% rmse(actual, ridpred),
  Lasso = cv_results %&gt;% rmse(actual, lassopred),
  ElasticNet = cv_results %&gt;% rmse(actual, elasticpred),
  SVR = cv_results %&gt;% rmse(actual, svrpred),
  PCR = cv_results %&gt;% rmse(actual, pcrpred),
  Cubist = cv_results %&gt;% rmse(actual, cubistpred),
  MARS = cv_results %&gt;% rmse(actual, marspred),
  MLinR = cv_results %&gt;% rmse(actual, lmpred)
  )

}

cv_train$RMSE &lt;- map(cv_train$splits, multiple_model)

cv_train &lt;- cv_train %&gt;%
  unnest(RMSE)

saveRDS(cv_train, &quot;cv_rsample.rds&quot;)</code></pre>
</div>
<div id="post-hoc-analysis" class="section level2">
<h2>Post-hoc Analysis</h2>
<p>Lets convert the RMSE dataset into tidy format and identify some statistics.</p>
<pre class="r"><code>stacked_rmse &lt;- cv_train %&gt;% 
  gather(key = &quot;model&quot;, value = &quot;statistic&quot;, -splits, -id, -id2) 

stacked_rmse %&gt;% 
  group_by(model) %&gt;% 
  summarise(Average = mean(statistic), 
            SD = sd(statistic), 
            Min = min(statistic), 
            Max = max(statistic)
            )</code></pre>
<pre><code>##    Average       SD     Min      Max
## 1 9243.058 1920.315 4179.81 14681.65</code></pre>
<p>Well, here we have quite a lot of information to infer decision.</p>
<p>Null Hypothesis :-</p>
<p>Alternate Hypothesis :-</p>
<pre class="r"><code>aov(log(statistic) ~ model, data = stacked_rmse) %&gt;% anova()</code></pre>
<pre><code>## Analysis of Variance Table
## 
## Response: log(statistic)
##            Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## model       9 17.478 1.94202  65.747 &lt; 2.2e-16 ***
## Residuals 990 29.242 0.02954                      
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1</code></pre>
<p>Lets graphically view these statistics.</p>
<pre class="r"><code>stacked_rmse %&gt;% 
  ggplot(aes(fct_reorder(model, statistic, median, .desc = TRUE), statistic, fill = model)) + 
  geom_boxplot(notch = TRUE) + 
  coord_flip() + 
  theme_classic() + 
  labs(x = &quot;Model&quot;, y = &quot;RMSE&quot;) +
  scale_fill_viridis_d(option = &quot;D&quot;) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="Part_IV_ML_files/figure-html/unnamed-chunk-21-1.png" width="672" /></p>
<p>We started with ten models, and are now left with five! One intresting point to note here is all Linear modelling techniques are dropped.</p>
<p>Although Cubist has the best performance but it some variations.</p>
<p>Lets have a different graphical view of our selected models.</p>
<pre class="r"><code>stacked_rmse %&gt;% 
  filter(!model %in% c(&quot;Ridge&quot;,&quot;MARS&quot;, &quot;PCR&quot;, &quot;MLinR&quot;, &quot;ElasticNet&quot;, &quot;Lasso&quot;, &quot;SVR&quot;)) %&gt;%
  ggplot(aes(x = fct_reorder(model, statistic, mean, .desc = TRUE), y = statistic, group = paste(id, id2), col = paste(id, id2))) + 
    geom_line(alpha = .75) + 
    theme(legend.position = &quot;none&quot;) +
    labs(x = &quot;Model&quot;, y = &quot;RMSE&quot;)</code></pre>
<p><img src="Part_IV_ML_files/figure-html/unnamed-chunk-22-1.png" width="672" /></p>
<pre class="r"><code>stacked_rmse %&gt;%  
  filter(model %in% c(&quot;Cubist&quot;, &quot;xgboost&quot;, &quot;randomforest&quot;)) %&gt;% 
  ggplot(aes(col = model, x = log(statistic))) + 
  geom_line(stat = &quot;density&quot;, trim = FALSE, size = 1) +
  scale_color_viridis_d() +
  theme_classic()</code></pre>
<p><img src="Part_IV_ML_files/figure-html/unnamed-chunk-23-1.png" width="672" /></p>
</div>
<div id="hyperparameters-tuning" class="section level2">
<h2>Hyperparameters tuning</h2>
<p>Now we have finalized five models, we will now tune the hyperparameters of these models and identify the best techniques, again using cross-validation.</p>
<pre class="r"><code>set.seed(42)

ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, savePredictions = TRUE, allowParallel = TRUE)</code></pre>
<div id="cubist-1" class="section level4">
<h4>Cubist</h4>
<p>At first iteration we use committees 1, 10, 50, 100.</p>
<pre class="r"><code>set.seed(42)

cub_grid &lt;- expand.grid(committees = c(50, 100),
                    neighbors = c(0, 1, 5, 9))

cubist_train &lt;- train(rec_obj, 
      data = nb_train,
      method = &quot;cubist&quot;,
      metric = &quot;RMSE&quot;, 
      trControl = ctrl,
      tuneGrid = cub_grid
      )

saveRDS(cubist_train, file = &quot;cubist_cv_model.rds&quot;) #7272.125
#cubist_train &lt;- readRDS(&quot;cubist_cv_model.rds&quot;)</code></pre>
<pre class="r"><code>cub_pred &lt;- cubist_train %&gt;% predict(as.matrix(train_data[x_col]))

tibble(
  Actual = train_data$HousePriceinK,
  cub_pred = cub_pred
) %&gt;% 
  rmse(Actual, cub_pred)</code></pre>
<pre><code>## [1] 4669.224</code></pre>
<p>When 100, 500 and 1000 are used 100 gives rmse 2305.542</p>
</div>
<div id="xgboost-1" class="section level4">
<h4>xgBoost</h4>
<pre class="r"><code>set.seed(42)

xgb_grid &lt;- expand.grid(
  nrounds = c(50, 75, 100),
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_train &lt;- train(rec_obj, 
      data = nb_train,
      method = &quot;xgbTree&quot;,
      metric = &quot;RMSE&quot;, 
      trControl = ctrl,
      tuneGrid = xgb_grid
      )
saveRDS(xgb_train, file = &quot;xgb_cv_model.rds&quot;)
#xgb_train &lt;- readRDS(&quot;xgb_cv_model.rds&quot;)</code></pre>
<pre class="r"><code>xgb_pred &lt;- xgb_train %&gt;% predict(train_data[x_col])

tibble(
  Actual = train_data$HousePriceinK,
  xgb_pred = xgb_pred
) %&gt;% 
  rmse(Actual, xgb_pred)</code></pre>
<pre><code>## [1] 2270.983</code></pre>
</div>
<div id="randomforest" class="section level4">
<h4>RandomForest</h4>
<pre class="r"><code>#start &lt;- Sys.time()

set.seed(42)

rf_grid &lt;- expand.grid(
  mtry = 3:8, # default value used was 6 #No of features to use
  splitrule = c(&quot;variance&quot;, &quot;extratrees&quot;, &quot;maxstat&quot;), #Rule on which split should be based on
  min.node.size = 5
)

rf_train &lt;- train(rec_obj, 
      data = nb_train,
      method = &quot;ranger&quot;,
      metric = &quot;RMSE&quot;, 
      trControl = ctrl,
      tuneGrid = rf_grid,
      num.trees = 500
      )
#(end &lt;- Sys.time() - start)

saveRDS(rf_train, file = &quot;rf_cv_model.rds&quot;)
#rf_train &lt;- readRDS(&quot;rf_cv_model.rds&quot;)</code></pre>
<pre class="r"><code>rf_pred &lt;- rf_train %&gt;% predict(train_data[x_col])

tibble(
  Actual = train_data$HousePriceinK,
  rf_pred = rf_pred
) %&gt;% 
  rmse(Actual, rf_pred)</code></pre>
<pre><code>## [1] 4066.907</code></pre>
<p>Group RMSE</p>
<pre class="r"><code>com_pred &lt;- tibble(
  Actual = train_data$HousePriceinK,
  cub_pred = cub_pred,
  xgb_pred = xgb_pred,
  rf_pred = rf_pred
)

optimize_RMSE &lt;- tibble(
  cubist = com_pred %&gt;% rmse(Actual, cub_pred),
  xgboost = com_pred %&gt;% rmse(Actual, xgb_pred),
  randomforest = com_pred %&gt;% rmse(Actual, rf_pred)
)

optimize_RMSE %&gt;% 
  gather(model, RMSE) %&gt;% 
  arrange(RMSE)</code></pre>
<pre><code>## # A tibble: 3 x 2
##   model         RMSE
##   &lt;chr&gt;        &lt;dbl&gt;
## 1 xgboost      2271.
## 2 randomforest 4067.
## 3 cubist       4669.</code></pre>
</div>
</div>
<div id="try-with-different-seeds" class="section level2">
<h2>Try with different seeds</h2>
<p>Now that we have drilled down to a single model, lets try it with different seeds.</p>
<pre class="r"><code>blank &lt;- tibble(Actual = train_data$HousePriceinK)

multiple_seed &lt;- function(seedno, recipe, data, ...){

  set.seed(seedno)

  xgb_train &lt;- train(recipe, 
      data = data,
      method = &quot;xgbTree&quot;,
      metric = &quot;RMSE&quot;
      #trControl = ctrl,
      #tuneGrid = xgb_grid
      )
  
  xgb_pred &lt;- xgb_train %&gt;% predict(train_data[1:35])
  
  blank %&gt;% 
    mutate(!!paste0(&quot;n&quot;, seedno) := xgb_pred)
  
  #saveRDS(xgb_train, file = paste0(&quot;xgb_cv_model&quot;,seedno,&quot;.rds&quot;))
  return(blank)
}

with_seeds &lt;- map(c(1, 100), multiple_seed, recipe = rec_obj, data = nb_data)</code></pre>
<pre class="r"><code>rmse_multiple_seed &lt;- as.data.frame(with_seeds) %&gt;% as.tibble() %&gt;% select(Actual, n1, n100, n500, n1000) %&gt;% mutate(n42 = xgb_train %&gt;% predict(train_data[x_col]))

rmse_various_seeds &lt;- tibble(
  n1 = rmse_multiple_seed %&gt;% rmse(Actual, n1),
  n42 = rmse_multiple_seed %&gt;% rmse(Actual, n42),
  n100 = rmse_multiple_seed %&gt;% rmse(Actual, n100),
  n500 = rmse_multiple_seed %&gt;% rmse(Actual, n500),
  n1000 = rmse_multiple_seed %&gt;% rmse(Actual, n1000)
)

rmse_various_seeds %&gt;% 
  gather(key = &quot;Seed_No&quot;, value = &quot;RMSE&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 2
##   Seed_No  RMSE
##   &lt;chr&gt;   &lt;dbl&gt;
## 1 n1      2721.
## 2 n42     2271.
## 3 n100    2581.
## 4 n500    2595.
## 5 n1000   2660.</code></pre>
</div>
<div id="final-model" class="section level2">
<h2>Final Model</h2>
<p>With above we conclude that xgBoost generalizes well on training dataset and is the final model that should be selected.</p>
<p>We will hot the final nail in the cofin by seeing how does it perform on unseen data.</p>
</div>
<div id="generalization" class="section level2">
<h2>Generalization</h2>
<pre class="r"><code>tibble(
  Actual = test_data %&gt;% pull(HousePriceinK),
  Prediction = xgb_train %&gt;% predict(test_data[x_col])
) %&gt;% 
  rmse(Actual, Prediction)</code></pre>
<pre><code>## [1] 2227.698</code></pre>
</div>
<div id="identify-important-varaibles." class="section level2">
<h2>Identify important varaibles.</h2>
<pre class="r"><code>library(DALEX); library(breakDown)

xgb_explainer &lt;- xgb_train %&gt;% 
  explain(label = &quot;xgb&quot;, data = train_data, y = train_data$HousePriceinK)

rf_explainer &lt;- rf_train %&gt;% 
  explain(label = &quot;rf&quot;, data = train_data, y = train_data$HousePriceinK)

cubist_explainer &lt;- cubist_train %&gt;% 
  explain(label = &quot;Cubist&quot;, data = train_data, y = train_data$HousePriceinK)

plot(model_performance(xgb_explainer), 
     model_performance(rf_explainer), 
     model_performance(cubist_explainer)
     )</code></pre>
<p><img src="Part_IV_ML_files/figure-html/unnamed-chunk-40-1.png" width="672" /></p>
<p>With the RMSE 2227, we can see that it generalizes well on unseen data.</p>
<p>Find Important features and reduce the features, i.e modify raw data accordingly.</p>
<pre class="r"><code>vi_xgb &lt;- xgb_explainer %&gt;% 
  variable_importance(loss_function = loss_root_mean_square)

vi_rf &lt;- rf_explainer %&gt;% 
  variable_importance(loss_function = loss_root_mean_square)

vi_cusit &lt;- cubist_explainer %&gt;% 
  variable_importance(loss_function = loss_root_mean_square)

plot(vi_xgb, vi_rf, vi_cusit)</code></pre>
<p><img src="Part_IV_ML_files/figure-html/unnamed-chunk-41-1.png" width="672" /></p>
<p>In all three models the top three influencive variables are Area, Lat, Long.</p>
<pre class="r"><code>a &lt;- xgb_explainer %&gt;% variable_response(variable =  &quot;Area&quot;, type = &quot;pdp&quot;)
b &lt;- xgb_explainer %&gt;% variable_response(variable =  &quot;Lat&quot;, type = &quot;pdp&quot;)
c &lt;- xgb_explainer %&gt;% variable_response(variable =  &quot;Long&quot;, type = &quot;pdp&quot;)</code></pre>
<p>Lets focus on the important variables again, but specifically with our model and individual variable performance.</p>
<p>As area increases the price of property increases.</p>
<pre class="r"><code>as.tibble(a) %&gt;%  
  ggplot(aes(x, y)) + 
  geom_line(col = &quot;darkgreen&quot;) +
  geom_point(col = &quot;darkgreen&quot;) + 
  labs(x = &quot;Area&quot;, y = &quot;Predicted Price&quot;, title = &quot;As area increases HousePrice increases&quot;, subtitle = &quot;Partial Dependency plot&quot;) +
  theme_mi2()</code></pre>
<p><img src="Part_IV_ML_files/figure-html/unnamed-chunk-44-1.png" width="672" /></p>
<p>As we move from South Mumbai to North Mumbai, House price declines. Note: 18.9 is south and 19.2 is North</p>
<pre class="r"><code>as.tibble(b) %&gt;%  
  ggplot(aes(x, y)) + 
  geom_line(col = &quot;darkgreen&quot;) +
  geom_point(col = &quot;darkgreen&quot;) + 
  geom_text(x = 18.98, y = 25000, label = &quot;South Mumbai&quot;, col = &quot;black&quot;, family = &quot;mono&quot;) +
  geom_text(x = 19.21, y = 15500, label = &quot;North Mumbai&quot;, col = &quot;black&quot;, family = &quot;mono&quot;) +
  labs(x = &quot;Lat&quot;, y = &quot;Predicted Price&quot;, title = &quot;As we move from South Mumbai to North Mumbai, House price declines&quot;, subtitle = &quot;Partial Dependency plot&quot;) +
  theme_mi2()</code></pre>
<p><img src="Part_IV_ML_files/figure-html/unnamed-chunk-45-1.png" width="672" /></p>
<p>As we move from west to east, House price declines. Not 72.80 is East and 72.95 is west - plot on graph.</p>
<pre class="r"><code>as.tibble(c) %&gt;%  
  ggplot(aes(x, y)) + 
  geom_line(col = &quot;darkgreen&quot;) +
  geom_point(col = &quot;darkgreen&quot;) + 
  geom_text(x = 72.80, y = 22500, label = &quot;West Mumbai&quot;, col = &quot;black&quot;, family = &quot;mono&quot;) +
  geom_text(x = 72.95, y = 14200, label = &quot;East Mumbai&quot;, col = &quot;black&quot;, family = &quot;mono&quot;) +
  labs(x = &quot;Lat&quot;, y = &quot;Predicted Price&quot;, title = &quot;As we move from West Mumbai to South Mumbai, House price declines&quot;, subtitle = &quot;Partial Dependency plot&quot;) + 
  theme_mi2()</code></pre>
<p><img src="Part_IV_ML_files/figure-html/unnamed-chunk-47-1.png" width="672" /></p>
<pre class="r"><code>single_variable(xgb_explainer, &quot;Area&quot;) %&gt;% plot()</code></pre>
<p><img src="Part_IV_ML_files/figure-html/unnamed-chunk-48-1.png" width="672" /></p>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
