<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8" />
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />




<title>Model Design</title>

<script src="site_libs/jquery-1.12.4/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/cosmo.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>
<script src="site_libs/htmlwidgets-1.2/htmlwidgets.js"></script>
<link href="site_libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="site_libs/datatables-binding-0.4/datatables.js"></script>
<link href="site_libs/dt-core-1.10.16/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="site_libs/dt-core-1.10.16/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="site_libs/dt-core-1.10.16/js/jquery.dataTables.min.js"></script>
<link href="site_libs/crosstalk-1.0.0/css/crosstalk.css" rel="stylesheet" />
<script src="site_libs/crosstalk-1.0.0/js/crosstalk.min.js"></script>

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 51px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 56px;
  margin-top: -56px;
}

.section h2 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h3 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h4 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h5 {
  padding-top: 56px;
  margin-top: -56px;
}
.section h6 {
  padding-top: 56px;
  margin-top: -56px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3,h4",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = false;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}

.tocify-subheader {
  display: inline;
}
.tocify-subheader .tocify-item {
  font-size: 0.95em;
}

</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Machine Learning - A tidy approach</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="nobroker-extract-data.html">Data Extract</a>
</li>
<li>
  <a href="Part_II.html">Data Cleaning</a>
</li>
<li>
  <a href="PartIVML.html">Model Design</a>
</li>
<li>
  <a href="stats.html">Linear Regression</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Model Design</h1>

</div>


<p>Here we will prepare various regression models for predicting house prices and choose a final model. We will start off in <code>tidymodel</code> style and later use the famous <code>caret</code> package to build model.</p>
<p>This is a long article as we follow a step by step detailed instruction. I would recommend not to miss the end part explaining black box model, a relatively new concept that helps us to decode black box models.</p>
<div id="which-model-to-select" class="section level2">
<h2>Which model to select?</h2>
<p>Borrowed from Aurélien Géron “Hands on ML” book, we will use the following steps to reach promising models.</p>
<ol style="list-style-type: decimal">
<li>Train many quick and dirty models from different categories using standard parameters.<br />
</li>
<li>Measure and compare their performance.<br />
</li>
<li>Analyze the most significant variables for each algorithm.</li>
<li>Have a quick round of feature selection and engineering.</li>
<li>Short-list the top three to five most promising models, preferring models that make different types of errors.</li>
</ol>
<pre class="r"><code>#plyr library is just to avoid conflicts with dplyr

library(plyr);library(tidyverse); library(caret); library(tidymodels); library(DT)

library(ranger); library(xgboost); library(glmnet); library(e1071); library(pls); library(Cubist); library(earth); library(Cubist); library(DALEX); library(breakDown)

nb_data &lt;- readRDS(&quot;nb_data.rds&quot;)</code></pre>
<p>Download nb_data.rds</p>
</div>
<div id="split-data" class="section level2">
<h2>Split Data</h2>
<p>Lets split our data in test and training set. We will use the rsample package for split.</p>
<pre class="r"><code>set.seed(42)

train_test_split &lt;- initial_split(nb_data)

nb_train &lt;- training(train_test_split)
nb_test &lt;- testing(train_test_split)</code></pre>
</div>
<div id="pre-process" class="section level2">
<h2>Pre Process</h2>
<p>Lets impute missing values. We already have the missing value information from data cleaning.</p>
<p>As we will prepare multiple models, lets create a structure for our model i.e. set preprocessing steps to our model.</p>
<pre class="r"><code>rec_obj &lt;- recipe(HousePriceinK ~ ., data = nb_train) %&gt;% #1
  step_knnimpute(all_predictors()) %&gt;%  #2
  step_dummy(all_predictors(), -all_numeric()) #3

prepare_rec &lt;- rec_obj %&gt;% prep(training = nb_train) #4

train_data &lt;- prepare_rec %&gt;% #5
  bake(nb_train) %&gt;% 
  select(-HousePriceinK, everything())

test_data &lt;- prepare_rec %&gt;% #5
  bake(nb_train) %&gt;% 
  select(-HousePriceinK, everything())

x_col &lt;- 1:39 #35
y_col &lt;- 40 #36</code></pre>
<ol style="list-style-type: decimal">
<li>Creating a structure for our model, the data argument is just to identify variables.<br />
</li>
<li>Impute missing value through KNN algorithm, the default value is 5.<br />
</li>
<li>Convert nominal data into numeric hot encoding.<br />
</li>
<li>Run reprocess step on our data.<br />
</li>
<li>Apply our prepossessing steps on training and test set.</li>
</ol>
<p>We can apply various preprocessing steps such as stp_center, step_scale, etc from the <a href="https://tidymodels.github.io/recipes/index.html">recipes</a> package.</p>
</div>
<div id="train-model" class="section level2">
<h2>Train Model</h2>
<p>We will train the following ten models; the applied predictive modeling book suggest to start from complex and move towards simpler modelling techniques.</p>
<ol style="list-style-type: decimal">
<li>RandomForest</li>
<li>xgBoost</li>
<li>Ridge Regression</li>
<li>Lasso Regression</li>
<li>ElasticNet Regression</li>
<li>Support vector regression</li>
<li>Principal Component Regression (PCR)</li>
<li>Cubist</li>
<li>Multivariate Adaptive Regression Splines (MARS)</li>
<li>Other (Polynomial?)</li>
<li>Linear Regression <a href="#stats">see here</a></li>
</ol>
<p>Click <a href="#compare_performance">here</a> to directly move to results instead!</p>
<div id="random-forest" class="section level4">
<h4>Random Forest</h4>
<p>Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions.</p>
<pre class="r"><code>library(ranger)

rf_model &lt;- ranger(HousePriceinK ~ ., data = train_data, num.trees = 500)

rf_prediction &lt;- rf_model %&gt;% 
  predict(train_data[x_col]) %&gt;% 
  predictions() </code></pre>
</div>
<div id="xgboost" class="section level4">
<h4>xgBoost</h4>
<p>Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor.</p>
<pre class="r"><code>library(xgboost)

xgb_model &lt;- xgboost(data = as.matrix(train_data[x_col]), label = as.matrix(train_data[y_col]), nrounds = 500, verbose = 0)

xg_prediction &lt;- xgb_model %&gt;% 
  predict(as.matrix(train_data[x_col]))</code></pre>
</div>
<div id="ridge-regression" class="section level4">
<h4>Ridge Regression</h4>
<pre class="r"><code>library(glmnet)

ridge_model &lt;- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 0)

ridge_predict&lt;- ridge_model %&gt;% 
  predict(as.matrix(train_data[x_col]), s = ridge_model$lambda.min) #s is panelty value</code></pre>
</div>
<div id="lasso-regression" class="section level4">
<h4>Lasso Regression</h4>
<pre class="r"><code>library(glmnet)

lasso_model &lt;- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 1)

lasso_predict&lt;- lasso_model %&gt;% 
  predict(as.matrix(train_data[x_col]), s = lasso_model$lambda.min)</code></pre>
</div>
<div id="elasticnet-regression" class="section level4">
<h4>ElasticNet Regression</h4>
<pre class="r"><code>elastic_model &lt;- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 0.5)

elastic_predict&lt;- elastic_model %&gt;% 
  predict(as.matrix(train_data[x_col]), s = elastic_model$lambda.min)</code></pre>
</div>
<div id="support-vector-regression" class="section level4">
<h4>Support Vector Regression</h4>
<pre class="r"><code>library(e1071)

svr_model &lt;- svm(HousePriceinK ~ ., data = train_data, type = &quot;eps-regression&quot;, kernel = &quot;radial&quot;)

svr_predict &lt;- svr_model %&gt;% 
  predict(train_data[x_col])</code></pre>
</div>
<div id="principal-component-regression-pcr" class="section level4">
<h4>Principal component regression (PCR)</h4>
<p>The principal components regression (PCR) constructs principal components (new and fewer variables); then use these components in a linear regression.</p>
<pre class="r"><code>library(pls)

pcr_model &lt;- pcr(HousePriceinK ~ ., data = train_data, scale = TRUE)

pcr_predict &lt;- pcr_model %&gt;% 
  predict(train_data[x_col], ncomp = 34)

#Using summary(pcr_model), we identify that with 34 ncomp 95% of X is retained.</code></pre>
</div>
<div id="cubist" class="section level4">
<h4>Cubist</h4>
<p>Like xgBoost, Cubist is a boosting technique plus it also performs neighbor based instance technique to get result.</p>
<pre class="r"><code>library(Cubist)

cubist_model &lt;- cubist(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), committees = 5)

cubist_predict &lt;- cubist_model %&gt;% 
  predict(as.matrix(train_data[x_col]))</code></pre>
</div>
<div id="multivariate-adaptive-regression-splines" class="section level4">
<h4>Multivariate Adaptive Regression Splines</h4>
<pre class="r"><code>library(earth)

mars_model &lt;- earth(HousePriceinK ~ ., data = train_data)

mars_predict &lt;- mars_model %&gt;% 
  predict(train_data[x_col])</code></pre>
</div>
<div id="other" class="section level4">
<h4>Other</h4>
<pre class="r"><code># lm_model &lt;- lm(HousePriceinK ~ ., data = train_data)
# 
# lm_predict &lt;- lm_model %&gt;% 
#   predict(train_data[x_col])</code></pre>
</div>
</div>
<div id="compare-performance" class="section level2">
<h2>Compare Performance</h2>
<p>For a regression problem, the Root Mean Square Error - RMSE is a best indicator of performance</p>
<pre class="r"><code># Collate all predicted values

results &lt;- tibble(
  Actual = train_data$HousePriceinK,
  RF = rf_prediction,
  xg = xg_prediction,
  ridge = ridge_predict %&gt;% as.vector(),
  lasso = lasso_predict %&gt;% as.vector(),
  elasticnet = elastic_predict %&gt;% as.vector(),
  svr = svr_predict %&gt;% as.vector(),
  PCR = pcr_predict %&gt;% as.vector(),
  cubist = cubist_predict,
  MARS = mars_predict %&gt;% as.vector()
  #MLR = lm_predict %&gt;% as.vector()
)

# Calculate RMSE from yardstick::rmse

RMSE &lt;- tibble(
  RMSE_RF = results %&gt;% rmse(Actual, RF),
  RMSE_XGB = results %&gt;% rmse(Actual, xg),
  RMSE_Ridge = results %&gt;% rmse(Actual, ridge),
  RMSE_Lasso = results %&gt;% rmse(Actual, lasso),
  RMSE_Elastic = results %&gt;% rmse(Actual, elasticnet),
  RMSE_SVR = results %&gt;% rmse(Actual, svr),
  RMSE_PCR = results %&gt;% rmse(Actual, PCR),
  RMSE_Cubist = results %&gt;% rmse(Actual, cubist),
  RMSE_MARS = results %&gt;% rmse(Actual, MARS)
  #RMSE_MLR = results %&gt;% rmse(Actual, MLR),
) %&gt;% 
  gather(key = &quot;Model&quot;, value = &quot;RMSE&quot;) %&gt;% 
  arrange(RMSE)

RMSE %&gt;% 
  datatable(rownames = FALSE, options = list(dom = &quot;t&quot;)) %&gt;% 
  formatRound(2)</code></pre>
<div id="htmlwidget-ed0e467c0528dc630c8c" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-ed0e467c0528dc630c8c">{"x":{"filter":"none","data":[["RMSE_XGB","RMSE_RF","RMSE_Cubist","RMSE_SVR","RMSE_MARS","RMSE_Lasso","RMSE_Elastic","RMSE_Ridge","RMSE_PCR"],[219.24273847885,4359.28211429299,7003.57684265237,8431.63949979694,9114.14598629255,9796.11889641595,9799.96074572569,9837.95917929192,10072.4973835592]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Model<\/th>\n      <th>RMSE<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-right","targets":1}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 2, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script>
<p>With default values of the model, we notice that xgBoost provides the best performance, but presumably it also overfits.</p>
<p>Next, lets use cross validation technique to evaluate over-fitting of models. Later on, we will tune hyper-parameters of models to check for better performance.</p>
</div>
<div id="cross-validation" class="section level2">
<h2>Cross Validation</h2>
<p>We will create a 10 cross validation set, each repeated 10 times. <a href="https://www.kaggle.com/dansbecker/cross-validation">Here</a> is a short introduction for cross validation.</p>
<pre class="r"><code>set.seed(42)

# Create 10 splits of data, repeat it for 5 times i.e create 50 samples.
cv_train &lt;- nb_train %&gt;% vfold_cv(v = 10, repeats = 10)

# First set
dim(cv_train$splits[[1]])</code></pre>
<pre><code>##   analysis assessment          n          p 
##       4482        499       4981         20</code></pre>
<p>In our train set we have 4,986 observation, with cross validation approx 4487 (90%) observation will be used to train and approx 499 (10%) observation will be used to test. This entire process will be repeated 10 times</p>
<pre class="r"><code># Function Start

multiple_model &lt;- function(split, ...){

cv_trainset &lt;- prepare_rec %&gt;% 
  bake(analysis(split)) %&gt;% 
  select(-HousePriceinK, everything())

cv_testset &lt;- prepare_rec %&gt;% 
  bake(assessment(split)) %&gt;% 
  select(-HousePriceinK)

# This looks conjusted, all we doing is repeating the single model with new cross validation data sets.

rfmod &lt;- ranger(HousePriceinK ~ ., data = cv_trainset, num.trees = 500)
xgmod &lt;- xgboost(data = as.matrix(cv_trainset[x_col]), label = as.matrix(cv_trainset[y_col]), nrounds = 500, verbose = 0)
ridgemod &lt;- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 0)
lassomod &lt;- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 1)
elasticmod &lt;- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 0.5)
svrmod &lt;- svm(HousePriceinK ~ ., data = cv_trainset, type = &quot;eps-regression&quot;, kernel = &quot;radial&quot;)
pcrmodel &lt;- pcr(HousePriceinK ~ ., data = cv_trainset)
cubistmod &lt;- cubist(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), committees = 5)
marsmod &lt;- earth(HousePriceinK ~ ., data = cv_trainset)
#lmmod &lt;- lm(HousePriceinK ~ ., data = cv_trainset)


# Store predictions on each cross validation samples

cv_results &lt;- tibble(
  actual = assessment(split) %&gt;% select(HousePriceinK) %&gt;% pull(),
  rfpred = rfmod %&gt;% predict(cv_testset) %&gt;% predictions(),
  xgpred = xgmod %&gt;% predict(as.matrix(cv_testset)),
  ridpred = ridgemod %&gt;% predict(as.matrix(cv_testset), s = ridgemod$lambda.min) %&gt;% as.vector(),
  lassopred = lassomod %&gt;% predict(as.matrix(cv_testset), s = lassomod$lambda.min) %&gt;% as.vector(),
  elasticpred = elasticmod %&gt;% predict(as.matrix(cv_testset), s = elasticmod$lambda.min) %&gt;% as.vector(),
  svrpred = svrmod %&gt;% predict(cv_testset) %&gt;% as.vector(),
  pcrpred = pcrmodel %&gt;% predict(cv_testset, ncomp = 34) %&gt;% as.vector(),
  cubistpred = cubistmod %&gt;% predict(cv_testset),
  marspred = marsmod %&gt;% predict(cv_testset) %&gt;% as.vector()
  #lmpred = lmmod %&gt;% predict(cv_testset) %&gt;% as.vector()

)

cv_RMSE &lt;- tibble(
  randomforest = cv_results %&gt;% rmse(actual, rfpred),
  xgboost = cv_results %&gt;% rmse(actual, xgpred),
  Ridge = cv_results %&gt;% rmse(actual, ridpred),
  Lasso = cv_results %&gt;% rmse(actual, lassopred),
  ElasticNet = cv_results %&gt;% rmse(actual, elasticpred),
  SVR = cv_results %&gt;% rmse(actual, svrpred),
  PCR = cv_results %&gt;% rmse(actual, pcrpred),
  Cubist = cv_results %&gt;% rmse(actual, cubistpred),
  MARS = cv_results %&gt;% rmse(actual, marspred)
  #MLinR = cv_results %&gt;% rmse(actual, lmpred)
  )

}

# Function End

# Run the multiple model function on each cross validation split
cv_train$RMSE &lt;- map(cv_train$splits, multiple_model)

cv_train &lt;- cv_train %&gt;%
  unnest(RMSE)

#saveRDS(cv_train, &quot;cv_rsample.rds&quot;)</code></pre>
</div>
<div id="post-hoc-analysis" class="section level2">
<h2>Post-hoc Analysis</h2>
<div id="model-statistics" class="section level3">
<h3>Model statistics</h3>
<p>We now have 50 RMSE scores, let’s analyze.</p>
<pre class="r"><code>stacked_rmse &lt;- cv_train %&gt;% 
  gather(key = &quot;model&quot;, value = &quot;statistic&quot;, -splits, -id, -id2) 

stacked_rmse %&gt;% 
  group_by(model) %&gt;% 
  summarise(Average = mean(statistic), 
            SD = sd(statistic), 
            Min = min(statistic), 
            Max = max(statistic)
            ) %&gt;% 
  datatable(rownames = FALSE, options = list(dom = &quot;t&quot;)) %&gt;% 
  DT::formatRound(2:5)</code></pre>
<div id="htmlwidget-6b48a4d6236dc6ef8592" style="width:100%;height:auto;" class="datatables html-widget"></div>
<script type="application/json" data-for="htmlwidget-6b48a4d6236dc6ef8592">{"x":{"filter":"none","data":[[9169.60158301322],[1949.94433006836],[4179.8103240219],[14681.6469895705]],"container":"<table class=\"display\">\n  <thead>\n    <tr>\n      <th>Average<\/th>\n      <th>SD<\/th>\n      <th>Min<\/th>\n      <th>Max<\/th>\n    <\/tr>\n  <\/thead>\n<\/table>","options":{"dom":"t","columnDefs":[{"className":"dt-right","targets":[0,1,2,3]}],"order":[],"autoWidth":false,"orderClasses":false,"rowCallback":"function(row, data) {\nDTWidget.formatRound(this, row, data, 1, 2, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 2, 2, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 3, 2, 3, ',', '.');\nDTWidget.formatRound(this, row, data, 4, 2, 3, ',', '.');\n}"}},"evals":["options.rowCallback"],"jsHooks":[]}</script>
</div>
<div id="graphical-representaion-of-models" class="section level3">
<h3>Graphical Representaion of models</h3>
<pre class="r"><code>stacked_rmse %&gt;% 
  ggplot(aes(fct_reorder(model, statistic, median, .desc = TRUE), statistic, fill = model)) + 
  geom_boxplot(notch = TRUE) + 
  coord_flip() + 
  theme_classic() + 
  labs(x = &quot;Model&quot;, y = &quot;RMSE&quot;) +
  scale_fill_viridis_d(option = &quot;D&quot;) +
  theme(legend.position = &quot;none&quot;)</code></pre>
<p><img src="PartIVML_files/figure-html/graph_one-1.png" width="672" /></p>
<p>From the above visualization we observe that cubist, xgboost and RandomForest are the top three models. Although Cubist has the best performance but it some variations.</p>
<p>For this project we will select this three models and optimize their performance to see if they can be improved further.</p>
<!--

Lets have a different graphical view of our selected models.


```r
stacked_rmse %>% 
  filter(!model %in% c("Ridge","MARS", "PCR", "ElasticNet", "Lasso", "SVR")) %>%
  ggplot(aes(x = fct_reorder(model, statistic, mean, .desc = TRUE), y = statistic, group = paste(id, id2), col = paste(id, id2))) + 
    geom_line(alpha = .75) + 
    theme(legend.position = "none") +
    labs(x = "Model", y = "RMSE")
```

<img src="PartIVML_files/figure-html/graph_two-1.png" width="672" />




```r
stacked_rmse %>%  
  filter(model %in% c("Cubist", "xgboost", "randomforest")) %>% 
  ggplot(aes(col = model, x = log(statistic))) + 
  geom_line(stat = "density", trim = FALSE, size = 1) +
  scale_color_viridis_d() +
  theme_classic()
```

<img src="PartIVML_files/figure-html/graph_three-1.png" width="672" />

-->
</div>
</div>
<div id="hyperparameters-tuning" class="section level2">
<h2>Hyperparameters tuning</h2>
<p>Various models needs manual parameters, like KNN needs number of nearest neighbor, randomForest needs number of trees to build. We need to tune this hyper-parameters and identify the best ones that optimize our results.</p>
<p>We will now tune the hyper-parameters of the selected three models to identify the best parameters.</p>
<div id="cross-validation-1" class="section level3">
<h3>Cross validation</h3>
<pre class="r"><code>set.seed(42)

ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 10, savePredictions = TRUE, allowParallel = TRUE)</code></pre>
<div id="cubist-1" class="section level4">
<h4>Cubist</h4>
<p>Cubist has two hyper-parameters, committees and neighbors.</p>
<pre class="r"><code>set.seed(42)

cub_grid &lt;- expand.grid(committees = c(50, 100),
                    neighbors = c(0, 1, 5, 9))

cubist_train &lt;- train(rec_obj, 
      data = nb_train,
      method = &quot;cubist&quot;,
      metric = &quot;RMSE&quot;, 
      trControl = ctrl,
      tuneGrid = cub_grid
      )

saveRDS(cubist_train, file = &quot;cubist_cv_model.rds&quot;) #7272.125
#cubist_train &lt;- readRDS(&quot;cubist_cv_model.rds&quot;)</code></pre>
<pre class="r"><code>cub_pred &lt;- cubist_train %&gt;% predict(as.matrix(train_data[x_col]))

tibble(
  Actual = train_data$HousePriceinK,
  cub_pred = cub_pred
) %&gt;% 
  rmse(Actual, cub_pred)</code></pre>
<pre><code>## [1] 4694.769</code></pre>
</div>
<div id="xgboost-1" class="section level4">
<h4>xgBoost</h4>
<p>The xgboost model has number of rounds, maximum depth to build, eta, gamma, column sample by tree, minimum child weight and subsample hyper-parameters.</p>
<pre class="r"><code>set.seed(42)

xgb_grid &lt;- expand.grid(
  nrounds = c(50, 75, 100),
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_train &lt;- train(rec_obj, 
      data = nb_train,
      method = &quot;xgbTree&quot;,
      metric = &quot;RMSE&quot;, 
      trControl = ctrl,
      tuneGrid = xgb_grid
      )
saveRDS(xgb_train, file = &quot;xgb_cv_model.rds&quot;)
#xgb_train &lt;- readRDS(&quot;xgb_cv_model.rds&quot;)</code></pre>
<pre class="r"><code>xgb_pred &lt;- xgb_train %&gt;% predict(train_data[x_col])

tibble(
  Actual = train_data$HousePriceinK,
  xgb_pred = xgb_pred
) %&gt;% 
  rmse(Actual, xgb_pred)</code></pre>
<pre><code>## [1] 2304.692</code></pre>
</div>
<div id="randomforest" class="section level4">
<h4>RandomForest</h4>
<p>RandomForest has mtry - number of features to use, splitrule and minimum node size hyper-parameters.</p>
<pre class="r"><code>#start &lt;- Sys.time()

set.seed(42)

rf_grid &lt;- expand.grid(
  mtry = 3:8, # default value used was 6 #No of features to use
  splitrule = c(&quot;variance&quot;, &quot;extratrees&quot;, &quot;maxstat&quot;), #Rule on which split should be based on
  min.node.size = 5
)

rf_train &lt;- train(rec_obj, 
      data = nb_train,
      method = &quot;ranger&quot;,
      metric = &quot;RMSE&quot;, 
      trControl = ctrl,
      tuneGrid = rf_grid,
      num.trees = 500
      )
#(end &lt;- Sys.time() - start)

saveRDS(rf_train, file = &quot;rf_cv_model.rds&quot;)
#rf_train &lt;- readRDS(&quot;rf_cv_model.rds&quot;)</code></pre>
<pre class="r"><code>rf_pred &lt;- rf_train %&gt;% predict(train_data[x_col])

tibble(
  Actual = train_data$HousePriceinK,
  rf_pred = rf_pred
) %&gt;% 
  rmse(Actual, rf_pred)</code></pre>
<pre><code>## [1] 4096.116</code></pre>
<p>Note: The <code>caret</code> package supports 256 models, the hyper parameters of these models can be found here.</p>
</div>
</div>
<div id="group-rmse" class="section level3">
<h3>Group RMSE</h3>
<pre class="r"><code>com_pred &lt;- tibble(
  Actual = train_data$HousePriceinK,
  cub_pred = cub_pred,
  xgb_pred = xgb_pred,
  rf_pred = rf_pred
)

optimize_RMSE &lt;- tibble(
  cubist = com_pred %&gt;% rmse(Actual, cub_pred),
  xgboost = com_pred %&gt;% rmse(Actual, xgb_pred),
  randomforest = com_pred %&gt;% rmse(Actual, rf_pred)
)

optimize_RMSE %&gt;% 
  gather(model, RMSE) %&gt;% 
  arrange(RMSE)</code></pre>
<pre><code>## # A tibble: 3 x 2
##   model         RMSE
##   &lt;chr&gt;        &lt;dbl&gt;
## 1 xgboost      2305.
## 2 randomforest 4096.
## 3 cubist       4695.</code></pre>
<p>In the above table we observe that after tuning the hyper-parameters of our models, xgboost has the best performance. Before we zero down on this model, lets try the model with different seeds.</p>
</div>
<div id="different-seeds" class="section level3">
<h3>Different seeds</h3>
<p>Remember, xgboost creates sample of our datasets. We can use seeds to reproduce our results. We will use various seeds to test if initial seed had any influence on our model.</p>
<pre class="r"><code>blank &lt;- tibble(Actual = train_data$HousePriceinK)

multiple_seed &lt;- function(seedno, recipe, data, ...){

  set.seed(seedno)

  xgb_train &lt;- train(recipe, 
      data = data,
      method = &quot;xgbTree&quot;,
      metric = &quot;RMSE&quot;
      #trControl = ctrl,
      #tuneGrid = xgb_grid
      )
  
  xgb_pred &lt;- xgb_train %&gt;% predict(train_data[1:35])
  
  blank %&gt;% 
    mutate(!!paste0(&quot;n&quot;, seedno) := xgb_pred)
  
  #saveRDS(xgb_train, file = paste0(&quot;xgb_cv_model&quot;,seedno,&quot;.rds&quot;))
  return(blank)
}

with_seeds &lt;- map(c(1, 100, 500, 1000), multiple_seed, recipe = rec_obj, data = nb_data)</code></pre>
<pre class="r"><code>rmse_multiple_seed &lt;- as.data.frame(with_seeds) %&gt;% as.tibble() %&gt;% select(Actual, n1, n100, n500, n1000) %&gt;% mutate(n42 = xgb_train %&gt;% predict(train_data[x_col]))

rmse_various_seeds &lt;- tibble(
  n1 = rmse_multiple_seed %&gt;% rmse(Actual, n1),
  n42 = rmse_multiple_seed %&gt;% rmse(Actual, n42),
  n100 = rmse_multiple_seed %&gt;% rmse(Actual, n100),
  n500 = rmse_multiple_seed %&gt;% rmse(Actual, n500),
  n1000 = rmse_multiple_seed %&gt;% rmse(Actual, n1000)
)

rmse_various_seeds %&gt;% 
  gather(key = &quot;Seed_No&quot;, value = &quot;RMSE&quot;)</code></pre>
<pre><code>## # A tibble: 5 x 2
##   Seed_No  RMSE
##   &lt;chr&gt;   &lt;dbl&gt;
## 1 n1      2721.
## 2 n42     2305.
## 3 n100    2581.
## 4 n500    2595.
## 5 n1000   2660.</code></pre>
</div>
</div>
<div id="final-model" class="section level2">
<h2>Final Model</h2>
<p>From above, we observe that xgBoost gives the best performance on training dataset. For now, we select it as our final model and use it on test data to evaluate generalization.</p>
<div id="generalization" class="section level3">
<h3>Generalization</h3>
<pre class="r"><code>tibble(
  Actual = test_data %&gt;% pull(HousePriceinK),
  Prediction = xgb_train %&gt;% predict(test_data[x_col])
) %&gt;% 
  rmse(Actual, Prediction)</code></pre>
<pre><code>## [1] 2254.172</code></pre>
<p>As the RMSE on the test data is similar to the one of train data, we can conclude that it generalize well on unseen data.</p>
</div>
</div>
<div id="explaining-black-box-models" class="section level2">
<h2>Explaining Black box models</h2>
<div id="identify-important-varaibles." class="section level3">
<h3>Identify important varaibles.</h3>
<p>Even though we have finalized our model, just for the sake of this post we will identify important variables of our top three models</p>
<pre class="r"><code>library(DALEX); library(breakDown)

xgb_explainer &lt;- xgb_train %&gt;% 
  explain(label = &quot;xgb&quot;, data = train_data, y = train_data$HousePriceinK)

rf_explainer &lt;- rf_train %&gt;% 
  explain(label = &quot;rf&quot;, data = train_data, y = train_data$HousePriceinK)

cubist_explainer &lt;- cubist_train %&gt;% 
  explain(label = &quot;Cubist&quot;, data = train_data, y = train_data$HousePriceinK)

# plot(model_performance(xgb_explainer), 
#      model_performance(rf_explainer), 
#      model_performance(cubist_explainer)
#      )</code></pre>
<pre class="r"><code>vi_xgb &lt;- xgb_explainer %&gt;% 
  variable_importance(loss_function = loss_root_mean_square)

vi_rf &lt;- rf_explainer %&gt;% 
  variable_importance(loss_function = loss_root_mean_square)

vi_cusit &lt;- cubist_explainer %&gt;% 
  variable_importance(loss_function = loss_root_mean_square)

plot(vi_xgb, vi_rf, vi_cusit)</code></pre>
<p><img src="PartIVML_files/figure-html/dalexExplainer-1.png" width="672" /></p>
<p>In all three models the top three important variables are Area, Lat, Long.</p>
</div>
<div id="individual-varaible-performance" class="section level3">
<h3>Individual varaible performance</h3>
<p>Lets focus on the important variables again, but specifically with our model and individual variable performance.</p>
<pre class="r"><code>a &lt;- xgb_explainer %&gt;% variable_response(variable =  &quot;Area&quot;, type = &quot;pdp&quot;)
b &lt;- xgb_explainer %&gt;% variable_response(variable =  &quot;Lat&quot;, type = &quot;pdp&quot;)
c &lt;- xgb_explainer %&gt;% variable_response(variable =  &quot;Long&quot;, type = &quot;pdp&quot;)</code></pre>
<div id="area" class="section level4">
<h4>Area</h4>
<pre class="r"><code>as.tibble(a) %&gt;%  
  ggplot(aes(x, y)) + 
  geom_line(col = &quot;darkgreen&quot;) +
  geom_point(col = &quot;darkgreen&quot;) + 
  labs(x = &quot;Area&quot;, y = &quot;Predicted Price&quot;, title = &quot;As area increases HousePrice increases&quot;, subtitle = &quot;Partial Dependency plot&quot;) +
  theme_mi2()</code></pre>
<p><img src="PartIVML_files/figure-html/AreaChart-1.png" width="672" /></p>
</div>
<div id="latitude" class="section level4">
<h4>Latitude</h4>
<p>As we move from South Mumbai to North Mumbai, House price declines. Note: 18.9 is south and 19.2 is North</p>
<pre class="r"><code>as.tibble(b) %&gt;%  
  ggplot(aes(x, y)) + 
  geom_line(col = &quot;darkgreen&quot;) +
  geom_point(col = &quot;darkgreen&quot;) + 
  geom_text(x = 18.98, y = 25000, label = &quot;South Mumbai&quot;, col = &quot;black&quot;, family = &quot;mono&quot;) +
  geom_text(x = 19.21, y = 15500, label = &quot;North Mumbai&quot;, col = &quot;black&quot;, family = &quot;mono&quot;) +
  labs(x = &quot;Lat&quot;, y = &quot;Predicted Price&quot;, title = &quot;As we move from South Mumbai to North Mumbai, House price declines&quot;, subtitle = &quot;Partial Dependency plot&quot;) +
  theme_mi2()</code></pre>
<p><img src="PartIVML_files/figure-html/SouthNorthChart-1.png" width="672" /></p>
</div>
<div id="longitude" class="section level4">
<h4>Longitude</h4>
<p>As we move from west to east, House price declines. Not 72.80 is East and 72.95 is west - plot on graph.</p>
<pre class="r"><code>as.tibble(c) %&gt;%  
  ggplot(aes(x, y)) + 
  geom_line(col = &quot;darkgreen&quot;) +
  geom_point(col = &quot;darkgreen&quot;) + 
  geom_text(x = 72.80, y = 22500, label = &quot;West Mumbai&quot;, col = &quot;black&quot;, family = &quot;mono&quot;) +
  geom_text(x = 72.95, y = 14200, label = &quot;East Mumbai&quot;, col = &quot;black&quot;, family = &quot;mono&quot;) +
  labs(x = &quot;Lat&quot;, y = &quot;Predicted Price&quot;, title = &quot;As we move from West Mumbai to South Mumbai, House price declines&quot;, subtitle = &quot;Partial Dependency plot&quot;) + 
  theme_mi2()</code></pre>
<p><img src="PartIVML_files/figure-html/WestEastChart-1.png" width="672" /></p>
<!--


```r
single_variable(xgb_explainer, "Area") %>% plot()
```

<img src="PartIVML_files/figure-html/lastplot-1.png" width="672" />

-->
</div>
</div>
</div>



</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
