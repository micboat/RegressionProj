---
title: "Part IV - ML"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r message=FALSE, warning=FALSE}

library(plyr);library(tidyverse); library(caret); library(tidymodels); library(ranger)

library(ranger); library(xgboost); library(glmnet); library(e1071); library(pls); library(Cubist); library(earth); library(Cubist)

nb_data <- readRDS("F:/Vasim/RegressionProj/nb_data.rds")


```

In this phase we will be preparing various regression models for predicting house prices.

## Which model to select?

Borrowed from Aurélien Géron "Hands on ML" book, we will use the following steps to reach promising models.

1) Train many quick and dirty models from different categories using standard parameters.  
2) Measure and compare their performance.  
3) Analyze the most significant variables for each algorithm.
4) Have a quick round of feature selection and engineering.
5) Short-list the top three to five most promising models, preferring models that make different types of errors.  

## Split Data

Lets split our data in test and training set. We will use the rsample package for split 

Note:- nb_data is already loaded in our workspace. It can be downloaded from here.

```{r}

set.seed(42)

train_test_split <- initial_split(nb_data)

nb_train <- training(train_test_split)
nb_test <- testing(train_test_split)


```

## Pre Process

Lets impute missing values. We alrady have the missing value stats with us (from module 1), lets recollect it.

| Features    | Missing values |
|-------------|----------------|
| FloorType   | 209            |
| Facing      | 1153           |
| PowerBackup | 4229           |
| WaterSupply | 116            |

Since we will run multiple models, first lets create a structure for our model i.e. set preprocessing steps to our model.

```{r}
rec_obj <- recipe(HousePriceinK ~ ., data = nb_train) %>% #1
  step_knnimpute(all_predictors()) %>%  #2
  #step_other(NoofBathrooms, NoofBalconies, threshold = 0.05) %>%
  step_dummy(all_predictors(), -all_numeric()) #%>% #3
  #step_log(all_outcomes()) %>% 
  #prep(training = nb_train) #4

#step_bs(Lat, Long)

prepare_rec <- rec_obj %>% prep(training = nb_train)

train_data <- prepare_rec %>% #5
  bake(nb_train) %>% 
  select(-HousePriceinK, everything())

test_data <- prepare_rec %>% #5
  bake(nb_train) %>% 
  select(-HousePriceinK, everything())

x_col <- 1:35
y_col <- 36

```

Let's understand what are we doing in above code!

1 - Creating a structure for our model, the data argument is just to identify variables.
2 - Impute missing value through KNN algorithim, the default K is 5.
3 - Convert nominal data into numeric hot encoding.
4 - Run preprocess step on our data
5 - Apply our preprossing steps on training and test set.

We can apply various preprocessing steps such as stp_center, step_scale, etc from the [recipes](https://tidymodels.github.io/recipes/index.html) package.

## Train Model

We will train the following ten models; the applied predictive modeling book suggest to start from complex and move towards simpler modelling techniques.

1)  RandomForest
2)  xgBoost
3)  Ridge Regression
4)  Lasso Regression
5)  ElasticNet Regression
6)  Support vector regression
7)  Principal Component Regression (PCR)
8)  Cubist
9)  Multivariate Adaptive Regression Splines (MARS)
10) Multiple Linear Regression

Click here to move to resutls


#### Random Forest - Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions.

```{r message=FALSE, warning=FALSE}
library(ranger)

rf_model <- ranger(HousePriceinK ~ ., data = train_data, num.trees = 500)

rf_prediction <- rf_model %>% 
  predict(train_data[x_col]) %>% 
  predictions() 

```

#### xgBoost - Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor.

```{r message=FALSE, warning=FALSE}
library(xgboost)

xgb_model <- xgboost(data = as.matrix(train_data[x_col]), label = as.matrix(train_data[y_col]), nrounds = 500, verbose = 0)
xg_prediction <- xgb_model %>% 
  predict(as.matrix(train_data[x_col]))

```

#### Ridge Regression

```{r message=FALSE, warning=FALSE}
library(glmnet)

ridge_model <- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 0)

ridge_predict<- ridge_model %>% 
  predict(as.matrix(train_data[x_col]), s = ridge_model$lambda.min) #s is panelty value

```

#### Lasso Regression

```{r message=FALSE, warning=FALSE}
library(glmnet)

lasso_model <- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 1)

lasso_predict<- lasso_model %>% 
  predict(as.matrix(train_data[x_col]), s = lasso_model$lambda.min)

```


#### ElasticNet Regression

```{r message=FALSE, warning=FALSE}
elastic_model <- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 0.5)

elastic_predict<- elastic_model %>% 
  predict(as.matrix(train_data[x_col]), s = elastic_model$lambda.min)


```


#### Support Vector Regression

```{r message=FALSE, warning=FALSE}
library(e1071)

svr_model <- svm(HousePriceinK ~ ., data = train_data, type = "eps-regression", kernel = "radial")

svr_predict <- svr_model %>% 
  predict(train_data[x_col])


```


#### Principal component regression (PCR) - The principal components regression (PCR) approach involves constructing principal components , and then using these components as the predictors in a linear regression model that is fit using least squares.

```{r message=FALSE, warning=FALSE}
library(pls)

pcr_model <- pcr(HousePriceinK ~ ., data = train_data, scale = TRUE)

pcr_predict <- pcr_model %>% 
  predict(train_data[x_col], ncomp = 34)

#Using summary(pcr_model), we identify that with 34 ncomp 95% of X is retained.

```


#### Cubist - Like xgBoost, Cubist is a boosting techinque plus it also performs neighbour based instance techinque to get result. 

```{r message=FALSE, warning=FALSE}
library(Cubist)

cubist_model <- cubist(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), committees = 5)

cubist_predict <- cubist_model %>% 
  predict(as.matrix(train_data[x_col]))

```

#### Multivariate Adaptive Regression Splines

```{r message=FALSE, warning=FALSE}
library(earth)

mars_model <- earth(HousePriceinK ~ ., data = train_data)

mars_predict <- mars_model %>% 
  predict(train_data[x_col])

```


#### Linear Regression

```{r message=FALSE, warning=FALSE}
lm_model <- lm(HousePriceinK ~ ., data = train_data)

lm_predict <- lm_model %>% 
  predict(train_data[x_col])
```


## Compare Performance

For a regression problem, the Root Mean Square Error - RMSE is a best indicator of performance

```{r}
# Since house price was transformed to log values, we convert it back using exp()
results <- tibble(
  Actual = train_data$HousePriceinK,
  RF = rf_prediction,
  xg = xg_prediction,
  ridge = ridge_predict %>% as.vector(),
  lasso = lasso_predict %>% as.vector(),
  elasticnet = elastic_predict %>% as.vector(),
  svr = svr_predict %>% as.vector(),
  PCR = pcr_predict %>% as.vector(),
  cubist = cubist_predict,
  MARS = mars_predict %>% as.vector(),
  MLR = lm_predict %>% as.vector()
)


RMSE <- tibble(
  RMSE_RF = results %>% rmse(Actual, RF),
  RMSE_XGB = results %>% rmse(Actual, xg),
  RMSE_Ridge = results %>% rmse(Actual, ridge),
  RMSE_Lasso = results %>% rmse(Actual, lasso),
  RMSE_Elastic = results %>% rmse(Actual, elasticnet),
  RMSE_SVR = results %>% rmse(Actual, svr),
  RMSE_PCR = results %>% rmse(Actual, PCR),
  RMSE_Cubist = results %>% rmse(Actual, cubist),
  RMSE_MARS = results %>% rmse(Actual, MARS),
  RMSE_MLR = results %>% rmse(Actual, MLR),
  
) %>% 
  gather("Model", 1:10, value = "RMSE") %>% 
  arrange(RMSE)

RMSE

```

```{r}
rm(results, lm_model, mars_model, lm_predict, mars_predict, cubist_predict, cubist_model, pcr_model, pcr_predict, svr_model, svr_predict, elastic_model, elastic_predict, lasso_model, lasso_predict, ridge_model, ridge_predict, xgb_model, xg_prediction, rf_model, rf_prediction, train_test_split)
```


With default values of the model, we can see that xgBoost turns out to provide the best performance. But these values do not provide a measurement of spread.

Next we will use cross validation technique to evaluate overfitting of models and imporve model performance. Further to that we will check if tuning model hyperparameters could help to increase performance.

Lets create cross validation samples of our data. We will create 10 cross validation set.

```{r}

set.seed(42)

#Create 10 splits of data, repeat it for 5 times i.e create 50 samples.
cv_train <- nb_train %>% vfold_cv(v = 10, repeats = 10)

first_sample <- cv_train$splits[[1]]

dim(first_sample)

```

As we see for a single sample we have total 4986 observation, this will create approx 4487 (90%) observation to perform training and approx 499 (10%) observation for testing purpose.

# Recipe should be applied on assessment set, so that blanks are imputed accordingly.


```{r}

multiple_model <- function(split, ...){

cv_trainset <- prepare_rec %>% bake(analysis(split)) %>% select(-HousePriceinK, everything())
cv_testset <- prepare_rec %>% bake(assessment(split)) %>% select(-HousePriceinK)

rfmod <- ranger(HousePriceinK ~ ., data = cv_trainset, num.trees = 500)
xgmod <- xgboost(data = as.matrix(cv_trainset[x_col]), label = as.matrix(cv_trainset[y_col]), nrounds = 500, verbose = 0)
ridgemod <- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 0)
lassomod <- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 1)
elasticmod <- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 0.5)
svrmod <- svm(HousePriceinK ~ ., data = cv_trainset, type = "eps-regression", kernel = "radial")
pcrmodel <- pcr(HousePriceinK ~ ., data = cv_trainset)
cubistmod <- cubist(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), committees = 5)
marsmod <- earth(HousePriceinK ~ ., data = cv_trainset)
lmmod <- lm(HousePriceinK ~ ., data = cv_trainset)

cv_results <- tibble(
  actual = assessment(split) %>% select(HousePriceinK) %>% pull(),
  rfpred = rfmod %>% predict(cv_testset) %>% predictions(),
  xgpred = xgmod %>% predict(as.matrix(cv_testset)),
  ridpred = ridgemod %>% predict(as.matrix(cv_testset), s = ridgemod$lambda.min) %>% as.vector(),
  lassopred = lassomod %>% predict(as.matrix(cv_testset), s = lassomod$lambda.min) %>% as.vector(),
  elasticpred = elasticmod %>% predict(as.matrix(cv_testset), s = elasticmod$lambda.min) %>% as.vector(),
  svrpred = svrmod %>% predict(cv_testset) %>% as.vector(),
  pcrpred = pcrmodel %>% predict(cv_testset, ncomp = 34) %>% as.vector(),
  cubistpred = cubistmod %>% predict(cv_testset),
  marspred = marsmod %>% predict(cv_testset) %>% as.vector(),
  lmpred = lmmod %>% predict(cv_testset) %>% as.vector()

)

cv_RMSE <- tibble(
  randomforest = cv_results %>% rmse(actual, rfpred),
  xgboost = cv_results %>% rmse(actual, xgpred),
  Ridge = cv_results %>% rmse(actual, ridpred),
  Lasso = cv_results %>% rmse(actual, lassopred),
  ElasticNet = cv_results %>% rmse(actual, elasticpred),
  SVR = cv_results %>% rmse(actual, svrpred),
  PCR = cv_results %>% rmse(actual, pcrpred),
  Cubist = cv_results %>% rmse(actual, cubistpred),
  MARS = cv_results %>% rmse(actual, marspred),
  MLinR = cv_results %>% rmse(actual, lmpred)
  )

}

cv_train$RMSE <- map(cv_train$splits, multiple_model)

cv_train <- cv_train %>%
  unnest(RMSE)

saveRDS(cv_train, "cv_rsample.rds")

cv_train <- readRDS("F:/Vasim/RegressionProj/cv_rsample.rds")

```

## POST HOC Analysis

Lets convert the RMSE dataset into tidy format and identify some statistics.

```{r}

stacked_rmse <- cv_train %>% 
  gather(key = "model", value = "statistic", -splits, -id, -id2) 

stacked_rmse %>% 
  group_by(model) %>% 
  summarise(Average = mean(statistic), 
            SD = sd(statistic), 
            Min = min(statistic), 
            Max = max(statistic)
            )
```

Well, here we have quite a lot of information to infer decision.

Null Hypothesis :- 

Alternate Hypothesis :- 

```{r}
aov(log(statistic) ~ model, data = stacked_rmse) %>% anova()
```

Lets graphically view these statistics.

```{r}
stacked_rmse %>% 
  ggplot(aes(fct_reorder(model, statistic, median, .desc = TRUE), statistic, fill = model)) + 
  geom_boxplot(notch = TRUE) + 
  coord_flip() + 
  theme_classic() + 
  labs(x = "Model", y = "RMSE") +
  scale_fill_viridis_d(option = "D") +
  theme(legend.position = "none")
```



We started with ten models, and are now left with five! One intresting point to note here is all Linear modelling techniques are dropped.

Although Cubist has the best performance but it some variations.

Lets have a different graphical view of our selected models.

```{r}

stacked_rmse %>% 
  filter(!model %in% c("Ridge","MARS", "PCR", "MLinR", "ElasticNet", "Lasso", "SVR")) %>%
  ggplot(aes(x = fct_reorder(model, statistic, mean, .desc = TRUE), y = statistic, group = paste(id, id2), col = paste(id, id2))) + 
    geom_line(alpha = .75) + 
    theme(legend.position = "none") +
    labs(x = "Model", y = "RMSE")
```



```{r}
stacked_rmse %>%  
  filter(model %in% c("Cubist", "xgboost", "randomforest")) %>% 
  ggplot(aes(col = model, x = log(statistic))) + 
  geom_line(stat = "density", trim = FALSE, size = 1) +
  scale_color_viridis_d() +
  theme_classic()

```


## Hyperparameters tuning

Now we have finalized five models, we will now tune the hyperparameters of these models and identify the best techniques, again using cross-validation.

```{r message=FALSE, warning=FALSE}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```


```{r}
set.seed(42)

ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10, savePredictions = TRUE, allowParallel = TRUE)

```


#### Cubist

At first iteration we use committees 1, 10, 50, 100.

```{r}
set.seed(42)

cub_grid <- expand.grid(committees = c(50, 100),
                    neighbors = c(0, 1, 5, 9))

cubist_train <- train(rec_obj, 
      data = nb_train,
      method = "cubist",
      metric = "RMSE", 
      trControl = ctrl,
      tuneGrid = cub_grid
      )

saveRDS(cubist_train, file = "cubist_cv_model.rds") #7272.125
#cubist_train <- readRDS("F:/Vasim/RegressionProj/cubist_cv_model.rds")

cub_pred <- cubist_train %>% predict(as.matrix(train_data[x_col]))

tibble(
  Actual = train_data$HousePriceinK,
  cub_pred = cub_pred
) %>% 
  rmse(Actual, cub_pred)

```

When 100, 500 and 1000 are used 100 gives rmse 2305.542

#### xgBoost

```{r}

set.seed(42)

xgb_grid <- expand.grid(
  nrounds = c(50, 75, 100),
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_train <- train(rec_obj, 
      data = nb_train,
      method = "xgbTree",
      metric = "RMSE", 
      trControl = ctrl,
      tuneGrid = xgb_grid
      )
saveRDS(xgb_train, file = "xgb_cv_model.rds") #2865.612 #3641.831 (42 seed)
#xgb_train <- readRDS("F:/Vasim/RegressionProj/xgb_cv_model.rds")

xgb_pred <- xgb_train %>% predict(train_data[x_col])

tibble(
  Actual = train_data$HousePriceinK,
  xgb_pred = xgb_pred
) %>% 
  rmse(Actual, xgb_pred)

```


#### RandomForest - Check with future %<-% package

```{r}
#start <- Sys.time()

set.seed(42)

rf_grid <- expand.grid(
  mtry = 3:8, # default value used was 6 #No of features to use
  splitrule = c("variance", "extratrees", "maxstat"), #Rule on which split should be based on
  min.node.size = 5
)

rf_train <- train(rec_obj, 
      data = nb_train,
      method = "ranger",
      metric = "RMSE", 
      trControl = ctrl,
      tuneGrid = rf_grid,
      num.trees = 500
      )
#(end <- Sys.time() - start)

saveRDS(rf_train, file = "rf_cv_model.rds") #5543.157
#rf_train <- readRDS("F:/Vasim/RegressionProj/rf_cv_model.rds")

rf_pred <- rf_train %>% predict(train_data[x_col])

tibble(
  Actual = train_data$HousePriceinK,
  rf_pred = rf_pred
) %>% 
  rmse(Actual, rf_pred)

```

Group RMSE

```{r}

com_pred <- tibble(
  Actual = train_data$HousePriceinK,
  cub_pred = cub_pred,
  xgb_pred = xgb_pred,
  rf_pred = rf_pred
)

optimize_RMSE <- tibble(
  cubist = com_pred %>% rmse(Actual, cub_pred),
  xgboost = com_pred %>% rmse(Actual, xgb_pred),
  randomforest = com_pred %>% rmse(Actual, rf_pred)
)

optimize_RMSE %>% 
  gather(model, RMSE) %>% 
  arrange(RMSE)
```


## Try with different seeds (use map function)

```{r}

blank <- tibble(Actual = train_data$HousePriceinK)

multiple_seed <- function(seedno, recipe, data, ...){

  set.seed(seedno)

  xgb_train <- train(recipe, 
      data = data,
      method = "xgbTree",
      metric = "RMSE"
      #trControl = ctrl,
      #tuneGrid = xgb_grid
      )
  
  xgb_pred <- xgb_train %>% predict(train_data[1:35])
  
  blank %>% 
    mutate(!!paste0("n", seedno) := xgb_pred)
  
  #Prepare a dataframe with seedno
  #saveRDS(xgb_train, file = paste0("xgb_cv_model",seedno,".rds"))
  return(blank)
}

with_seeds <- map(c(1, 100), multiple_seed, recipe = rec_obj, data = nb_data)

#xgb_pred <- xgb_train %>% predict(train_data[x_col])

tibble(
  Actual = train_data$HousePriceinK,
  xgb_pred = xgb_pred
) %>% 
  rmse(Actual, xgb_pred)

```




## Selected Models

Fimd Important features and reduce the features, i.e modify raw data accordingly.


## Final Model


## Generalization

<!--
```{r}
#library(randomForest)

randomForest(HousePriceinK ~ ., data = train_data)

```


How many threads to use

```{r message=FALSE, warning=FALSE}
library(h2o)
h2o.init()

h2o.randomForest(y = "HousePriceinK",
                 training_frame = as.h2o(train_data))


```

```{r}
h2o.shutdown()
```



```{r}
set.seed(42)

train(HousePriceinK ~ ., 
      data = train_data,
      method = "ranger"
      
      )

```

-->
