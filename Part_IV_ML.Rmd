---
title: "Model Design"
output: 
  html_document:
    toc: true
    toc_depth: 4
    collapsed: true
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

```{r message=FALSE, warning=FALSE}

library(plyr);library(tidyverse); library(caret); library(tidymodels); library(ranger)

library(ranger); library(xgboost); library(glmnet); library(e1071); library(pls); library(Cubist); library(earth); library(Cubist); library(DALEX); library(breakDown)

nb_data <- readRDS("nb_data.rds")


```

In this phase we will be preparing various regression models for predicting house prices.

## Which model to select?

Borrowed from Aurélien Géron "Hands on ML" book, we will use the following steps to reach promising models.

1) Train many quick and dirty models from different categories using standard parameters.  
2) Measure and compare their performance.  
3) Analyze the most significant variables for each algorithm.
4) Have a quick round of feature selection and engineering.
5) Short-list the top three to five most promising models, preferring models that make different types of errors.  

## Split Data

Lets split our data in test and training set. We will use the rsample package for split 

Note:- nb_data is already loaded in our workspace. It can be downloaded from here.

```{r}

set.seed(42)

train_test_split <- initial_split(nb_data)

nb_train <- training(train_test_split)
nb_test <- testing(train_test_split)


```

## Pre Process

Lets impute missing values. We alrady have the missing value stats with us (from module 1), lets recollect it.

| Features    | Missing values |
|-------------|----------------|
| FloorType   | 209            |
| Facing      | 1153           |
| PowerBackup | 4229           |
| WaterSupply | 116            |

Since we will run multiple models, first lets create a structure for our model i.e. set preprocessing steps to our model.

```{r}
rec_obj <- recipe(HousePriceinK ~ ., data = nb_train) %>% #1
  step_knnimpute(all_predictors()) %>%  #2
  #step_other(NoofBathrooms, NoofBalconies, threshold = 0.05) %>%
  step_dummy(all_predictors(), -all_numeric()) #%>% #3
  #step_log(all_outcomes()) %>% 
  #prep(training = nb_train) #4

#step_bs(Lat, Long)

prepare_rec <- rec_obj %>% prep(training = nb_train)

train_data <- prepare_rec %>% #5
  bake(nb_train) %>% 
  select(-HousePriceinK, everything())

test_data <- prepare_rec %>% #5
  bake(nb_train) %>% 
  select(-HousePriceinK, everything())

x_col <- 1:35
y_col <- 36

```

Let's understand what are we doing in above code!

1 - Creating a structure for our model, the data argument is just to identify variables.
2 - Impute missing value through KNN algorithim, the default K is 5.
3 - Convert nominal data into numeric hot encoding.
4 - Run preprocess step on our data
5 - Apply our preprossing steps on training and test set.

We can apply various preprocessing steps such as stp_center, step_scale, etc from the [recipes](https://tidymodels.github.io/recipes/index.html) package.

## Train Model

We will train the following ten models; the applied predictive modeling book suggest to start from complex and move towards simpler modelling techniques.

1)  RandomForest
2)  xgBoost
3)  Ridge Regression
4)  Lasso Regression
5)  ElasticNet Regression
6)  Support vector regression
7)  Principal Component Regression (PCR)
8)  Cubist
9)  Multivariate Adaptive Regression Splines (MARS)
10) Multiple Linear Regression

Click here to move to resutls


#### Random Forest
Random Forests work by training many Decision Trees on random subsets of the features, then averaging out their predictions.

```{r message=FALSE, warning=FALSE}
library(ranger)

rf_model <- ranger(HousePriceinK ~ ., data = train_data, num.trees = 500)

rf_prediction <- rf_model %>% 
  predict(train_data[x_col]) %>% 
  predictions() 

```

#### xgBoost
Gradient Boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor.

```{r message=FALSE, warning=FALSE}
library(xgboost)

xgb_model <- xgboost(data = as.matrix(train_data[x_col]), label = as.matrix(train_data[y_col]), nrounds = 500, verbose = 0)
xg_prediction <- xgb_model %>% 
  predict(as.matrix(train_data[x_col]))

```

#### Ridge Regression

```{r message=FALSE, warning=FALSE}
library(glmnet)

ridge_model <- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 0)

ridge_predict<- ridge_model %>% 
  predict(as.matrix(train_data[x_col]), s = ridge_model$lambda.min) #s is panelty value

```

#### Lasso Regression

```{r message=FALSE, warning=FALSE}
library(glmnet)

lasso_model <- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 1)

lasso_predict<- lasso_model %>% 
  predict(as.matrix(train_data[x_col]), s = lasso_model$lambda.min)

```


#### ElasticNet Regression

```{r message=FALSE, warning=FALSE}
elastic_model <- cv.glmnet(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), alpha = 0.5)

elastic_predict<- elastic_model %>% 
  predict(as.matrix(train_data[x_col]), s = elastic_model$lambda.min)


```


#### Support Vector Regression

```{r message=FALSE, warning=FALSE}
library(e1071)

svr_model <- svm(HousePriceinK ~ ., data = train_data, type = "eps-regression", kernel = "radial")

svr_predict <- svr_model %>% 
  predict(train_data[x_col])


```


#### Principal component regression (PCR)
The principal components regression (PCR) approach involves constructing principal components , and then using these components as the predictors in a linear regression model that is fit using least squares.

```{r message=FALSE, warning=FALSE}
library(pls)

pcr_model <- pcr(HousePriceinK ~ ., data = train_data, scale = TRUE)

pcr_predict <- pcr_model %>% 
  predict(train_data[x_col], ncomp = 34)

#Using summary(pcr_model), we identify that with 34 ncomp 95% of X is retained.

```


#### Cubist
Like xgBoost, Cubist is a boosting techinque plus it also performs neighbour based instance techinque to get result. 

```{r message=FALSE, warning=FALSE}
library(Cubist)

cubist_model <- cubist(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), committees = 5)

cubist_predict <- cubist_model %>% 
  predict(as.matrix(train_data[x_col]))

```

#### Multivariate Adaptive Regression Splines

```{r message=FALSE, warning=FALSE}
library(earth)

mars_model <- earth(HousePriceinK ~ ., data = train_data)

mars_predict <- mars_model %>% 
  predict(train_data[x_col])

```


#### Linear Regression

```{r message=FALSE, warning=FALSE}
lm_model <- lm(HousePriceinK ~ ., data = train_data)

lm_predict <- lm_model %>% 
  predict(train_data[x_col])
```


## Compare Performance

For a regression problem, the Root Mean Square Error - RMSE is a best indicator of performance

```{r}
# Since house price was transformed to log values, we convert it back using exp()
results <- tibble(
  Actual = train_data$HousePriceinK,
  RF = rf_prediction,
  xg = xg_prediction,
  ridge = ridge_predict %>% as.vector(),
  lasso = lasso_predict %>% as.vector(),
  elasticnet = elastic_predict %>% as.vector(),
  svr = svr_predict %>% as.vector(),
  PCR = pcr_predict %>% as.vector(),
  cubist = cubist_predict,
  MARS = mars_predict %>% as.vector(),
  MLR = lm_predict %>% as.vector()
)


RMSE <- tibble(
  RMSE_RF = results %>% rmse(Actual, RF),
  RMSE_XGB = results %>% rmse(Actual, xg),
  RMSE_Ridge = results %>% rmse(Actual, ridge),
  RMSE_Lasso = results %>% rmse(Actual, lasso),
  RMSE_Elastic = results %>% rmse(Actual, elasticnet),
  RMSE_SVR = results %>% rmse(Actual, svr),
  RMSE_PCR = results %>% rmse(Actual, PCR),
  RMSE_Cubist = results %>% rmse(Actual, cubist),
  RMSE_MARS = results %>% rmse(Actual, MARS),
  RMSE_MLR = results %>% rmse(Actual, MLR),
  
) %>% 
  gather("Model", 1:10, value = "RMSE") %>% 
  arrange(RMSE)

RMSE

```

```{r}
rm(results, lm_model, mars_model, lm_predict, mars_predict, cubist_predict, cubist_model, pcr_model, pcr_predict, svr_model, svr_predict, elastic_model, elastic_predict, lasso_model, lasso_predict, ridge_model, ridge_predict, xgb_model, xg_prediction, rf_model, rf_prediction, train_test_split)
```


With default values of the model, we can see that xgBoost turns out to provide the best performance. But these values do not provide a measurement of spread.

Next we will use cross validation technique to evaluate overfitting of models and imporve model performance. Further to that we will check if tuning model hyperparameters could help to increase performance.

## Cross Validation

Lets create cross validation samples of our data. We will create 10 cross validation set.

```{r}

set.seed(42)

#Create 10 splits of data, repeat it for 5 times i.e create 50 samples.
cv_train <- nb_train %>% vfold_cv(v = 10, repeats = 10)

first_sample <- cv_train$splits[[1]]

dim(first_sample)

```

As we see for a single sample we have total 4986 observation, this will create approx 4487 (90%) observation to perform training and approx 499 (10%) observation for testing purpose.

Recipe should be applied on assessment set, so that blanks are imputed accordingly.

```{r, echo=FALSE}
cv_train <- readRDS("cv_rsample.rds")
```


```{r, eval=FALSE}

multiple_model <- function(split, ...){

cv_trainset <- prepare_rec %>% bake(analysis(split)) %>% select(-HousePriceinK, everything())
cv_testset <- prepare_rec %>% bake(assessment(split)) %>% select(-HousePriceinK)

rfmod <- ranger(HousePriceinK ~ ., data = cv_trainset, num.trees = 500)
xgmod <- xgboost(data = as.matrix(cv_trainset[x_col]), label = as.matrix(cv_trainset[y_col]), nrounds = 500, verbose = 0)
ridgemod <- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 0)
lassomod <- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 1)
elasticmod <- cv.glmnet(x = as.matrix(cv_trainset[x_col]), y = as.matrix(cv_trainset[y_col]), alpha = 0.5)
svrmod <- svm(HousePriceinK ~ ., data = cv_trainset, type = "eps-regression", kernel = "radial")
pcrmodel <- pcr(HousePriceinK ~ ., data = cv_trainset)
cubistmod <- cubist(x = as.matrix(train_data[x_col]), y = as.matrix(train_data[y_col]), committees = 5)
marsmod <- earth(HousePriceinK ~ ., data = cv_trainset)
lmmod <- lm(HousePriceinK ~ ., data = cv_trainset)

cv_results <- tibble(
  actual = assessment(split) %>% select(HousePriceinK) %>% pull(),
  rfpred = rfmod %>% predict(cv_testset) %>% predictions(),
  xgpred = xgmod %>% predict(as.matrix(cv_testset)),
  ridpred = ridgemod %>% predict(as.matrix(cv_testset), s = ridgemod$lambda.min) %>% as.vector(),
  lassopred = lassomod %>% predict(as.matrix(cv_testset), s = lassomod$lambda.min) %>% as.vector(),
  elasticpred = elasticmod %>% predict(as.matrix(cv_testset), s = elasticmod$lambda.min) %>% as.vector(),
  svrpred = svrmod %>% predict(cv_testset) %>% as.vector(),
  pcrpred = pcrmodel %>% predict(cv_testset, ncomp = 34) %>% as.vector(),
  cubistpred = cubistmod %>% predict(cv_testset),
  marspred = marsmod %>% predict(cv_testset) %>% as.vector(),
  lmpred = lmmod %>% predict(cv_testset) %>% as.vector()

)

cv_RMSE <- tibble(
  randomforest = cv_results %>% rmse(actual, rfpred),
  xgboost = cv_results %>% rmse(actual, xgpred),
  Ridge = cv_results %>% rmse(actual, ridpred),
  Lasso = cv_results %>% rmse(actual, lassopred),
  ElasticNet = cv_results %>% rmse(actual, elasticpred),
  SVR = cv_results %>% rmse(actual, svrpred),
  PCR = cv_results %>% rmse(actual, pcrpred),
  Cubist = cv_results %>% rmse(actual, cubistpred),
  MARS = cv_results %>% rmse(actual, marspred),
  MLinR = cv_results %>% rmse(actual, lmpred)
  )

}

cv_train$RMSE <- map(cv_train$splits, multiple_model)

cv_train <- cv_train %>%
  unnest(RMSE)

saveRDS(cv_train, "cv_rsample.rds")

```

## Post-hoc Analysis

Lets convert the RMSE dataset into tidy format and identify some statistics.

```{r}

stacked_rmse <- cv_train %>% 
  gather(key = "model", value = "statistic", -splits, -id, -id2) 

stacked_rmse %>% 
  group_by(model) %>% 
  summarise(Average = mean(statistic), 
            SD = sd(statistic), 
            Min = min(statistic), 
            Max = max(statistic)
            )
```

Well, here we have quite a lot of information to infer decision.

Null Hypothesis :- 

Alternate Hypothesis :- 

```{r}
aov(log(statistic) ~ model, data = stacked_rmse) %>% anova()
```

Lets graphically view these statistics.

```{r}
stacked_rmse %>% 
  ggplot(aes(fct_reorder(model, statistic, median, .desc = TRUE), statistic, fill = model)) + 
  geom_boxplot(notch = TRUE) + 
  coord_flip() + 
  theme_classic() + 
  labs(x = "Model", y = "RMSE") +
  scale_fill_viridis_d(option = "D") +
  theme(legend.position = "none")
```



We started with ten models, and are now left with five! One intresting point to note here is all Linear modelling techniques are dropped.

Although Cubist has the best performance but it some variations.

Lets have a different graphical view of our selected models.

```{r}

stacked_rmse %>% 
  filter(!model %in% c("Ridge","MARS", "PCR", "MLinR", "ElasticNet", "Lasso", "SVR")) %>%
  ggplot(aes(x = fct_reorder(model, statistic, mean, .desc = TRUE), y = statistic, group = paste(id, id2), col = paste(id, id2))) + 
    geom_line(alpha = .75) + 
    theme(legend.position = "none") +
    labs(x = "Model", y = "RMSE")
```



```{r}
stacked_rmse %>%  
  filter(model %in% c("Cubist", "xgboost", "randomforest")) %>% 
  ggplot(aes(col = model, x = log(statistic))) + 
  geom_line(stat = "density", trim = FALSE, size = 1) +
  scale_color_viridis_d() +
  theme_classic()

```


## Hyperparameters tuning

Now we have finalized five models, we will now tune the hyperparameters of these models and identify the best techniques, again using cross-validation.

```{r message=FALSE, warning=FALSE, echo = FALSE, eval=FALSE}
library(parallel)
library(doParallel)
cluster <- makeCluster(detectCores() - 1) # convention to leave 1 core for OS
registerDoParallel(cluster)
```


```{r}
set.seed(42)

ctrl <- trainControl(method = "repeatedcv", number = 10, repeats = 10, savePredictions = TRUE, allowParallel = TRUE)

```


#### Cubist

At first iteration we use committees 1, 10, 50, 100.
```{r, echo = FALSE}
cubist_train <- readRDS("cubist_cv_model.rds")


```


```{r, eval=FALSE}
set.seed(42)

cub_grid <- expand.grid(committees = c(50, 100),
                    neighbors = c(0, 1, 5, 9))

cubist_train <- train(rec_obj, 
      data = nb_train,
      method = "cubist",
      metric = "RMSE", 
      trControl = ctrl,
      tuneGrid = cub_grid
      )

saveRDS(cubist_train, file = "cubist_cv_model.rds") #7272.125
#cubist_train <- readRDS("cubist_cv_model.rds")

```

```{r}
cub_pred <- cubist_train %>% predict(as.matrix(train_data[x_col]))

tibble(
  Actual = train_data$HousePriceinK,
  cub_pred = cub_pred
) %>% 
  rmse(Actual, cub_pred)

```

When 100, 500 and 1000 are used 100 gives rmse 2305.542

#### xgBoost

```{r, echo = FALSE}
xgb_train <- readRDS("xgb_cv_model.rds")
```


```{r, eval = FALSE}

set.seed(42)

xgb_grid <- expand.grid(
  nrounds = c(50, 75, 100),
  max_depth = 6,
  eta = 0.3,
  gamma = 0,
  colsample_bytree = 1,
  min_child_weight = 1,
  subsample = 1
)

xgb_train <- train(rec_obj, 
      data = nb_train,
      method = "xgbTree",
      metric = "RMSE", 
      trControl = ctrl,
      tuneGrid = xgb_grid
      )
saveRDS(xgb_train, file = "xgb_cv_model.rds")
#xgb_train <- readRDS("xgb_cv_model.rds")
```

```{r}
xgb_pred <- xgb_train %>% predict(train_data[x_col])

tibble(
  Actual = train_data$HousePriceinK,
  xgb_pred = xgb_pred
) %>% 
  rmse(Actual, xgb_pred)

```


#### RandomForest

```{r, echo=FALSE}
rf_train <- readRDS("rf_cv_model.rds")
```


```{r, eval = FALSE}
#start <- Sys.time()

set.seed(42)

rf_grid <- expand.grid(
  mtry = 3:8, # default value used was 6 #No of features to use
  splitrule = c("variance", "extratrees", "maxstat"), #Rule on which split should be based on
  min.node.size = 5
)

rf_train <- train(rec_obj, 
      data = nb_train,
      method = "ranger",
      metric = "RMSE", 
      trControl = ctrl,
      tuneGrid = rf_grid,
      num.trees = 500
      )
#(end <- Sys.time() - start)

saveRDS(rf_train, file = "rf_cv_model.rds")
#rf_train <- readRDS("rf_cv_model.rds")
```

```{r}
rf_pred <- rf_train %>% predict(train_data[x_col])

tibble(
  Actual = train_data$HousePriceinK,
  rf_pred = rf_pred
) %>% 
  rmse(Actual, rf_pred)

```

Group RMSE

```{r}

com_pred <- tibble(
  Actual = train_data$HousePriceinK,
  cub_pred = cub_pred,
  xgb_pred = xgb_pred,
  rf_pred = rf_pred
)

optimize_RMSE <- tibble(
  cubist = com_pred %>% rmse(Actual, cub_pred),
  xgboost = com_pred %>% rmse(Actual, xgb_pred),
  randomforest = com_pred %>% rmse(Actual, rf_pred)
)

optimize_RMSE %>% 
  gather(model, RMSE) %>% 
  arrange(RMSE)
```


## Try with different seeds
Now that we have drilled down to a single model, lets try it with different seeds.

```{r, echo = FALSE}
with_seeds <- readRDS("with_seeds_model.rds")
```

```{r, eval = FALSE}

blank <- tibble(Actual = train_data$HousePriceinK)

multiple_seed <- function(seedno, recipe, data, ...){

  set.seed(seedno)

  xgb_train <- train(recipe, 
      data = data,
      method = "xgbTree",
      metric = "RMSE"
      #trControl = ctrl,
      #tuneGrid = xgb_grid
      )
  
  xgb_pred <- xgb_train %>% predict(train_data[1:35])
  
  blank %>% 
    mutate(!!paste0("n", seedno) := xgb_pred)
  
  #saveRDS(xgb_train, file = paste0("xgb_cv_model",seedno,".rds"))
  return(blank)
}

with_seeds <- map(c(1, 100), multiple_seed, recipe = rec_obj, data = nb_data)


```

```{r}
rmse_multiple_seed <- as.data.frame(with_seeds) %>% as.tibble() %>% select(Actual, n1, n100, n500, n1000) %>% mutate(n42 = xgb_train %>% predict(train_data[x_col]))

rmse_various_seeds <- tibble(
  n1 = rmse_multiple_seed %>% rmse(Actual, n1),
  n42 = rmse_multiple_seed %>% rmse(Actual, n42),
  n100 = rmse_multiple_seed %>% rmse(Actual, n100),
  n500 = rmse_multiple_seed %>% rmse(Actual, n500),
  n1000 = rmse_multiple_seed %>% rmse(Actual, n1000)
)

rmse_various_seeds %>% 
  gather(key = "Seed_No", value = "RMSE")
```


## Final Model

With above we conclude that xgBoost generalizes well on training dataset and is the final model that should be selected.

We will hot the final nail in the cofin by seeing how does it perform on unseen data.


## Generalization

```{r}

tibble(
  Actual = test_data %>% pull(HousePriceinK),
  Prediction = xgb_train %>% predict(test_data[x_col])
) %>% 
  rmse(Actual, Prediction)
```

## Identify important varaibles.

```{r}
library(DALEX); library(breakDown)

xgb_explainer <- xgb_train %>% 
  explain(label = "xgb", data = train_data, y = train_data$HousePriceinK)

rf_explainer <- rf_train %>% 
  explain(label = "rf", data = train_data, y = train_data$HousePriceinK)

cubist_explainer <- cubist_train %>% 
  explain(label = "Cubist", data = train_data, y = train_data$HousePriceinK)

plot(model_performance(xgb_explainer), 
     model_performance(rf_explainer), 
     model_performance(cubist_explainer)
     )
```

With the RMSE 2227, we can see that it generalizes well on unseen data.


Find Important features and reduce the features, i.e modify raw data accordingly.

```{r}

vi_xgb <- xgb_explainer %>% 
  variable_importance(loss_function = loss_root_mean_square)

vi_rf <- rf_explainer %>% 
  variable_importance(loss_function = loss_root_mean_square)

vi_cusit <- cubist_explainer %>% 
  variable_importance(loss_function = loss_root_mean_square)

plot(vi_xgb, vi_rf, vi_cusit)

```

In all three models the top three influencive variables are Area, Lat, Long.

```{r, echo = FALSE, eval = FALSE}
prediction_breakdown(xgb_explainer, train_data[1,-36]) %>% plot()
```


```{r}

a <- xgb_explainer %>% variable_response(variable =  "Area", type = "pdp")
b <- xgb_explainer %>% variable_response(variable =  "Lat", type = "pdp")
c <- xgb_explainer %>% variable_response(variable =  "Long", type = "pdp")

```


Lets focus on the important variables again, but specifically with our model and individual variable performance.

As area increases the price of property increases.

```{r}
as.tibble(a) %>%  
  ggplot(aes(x, y)) + 
  geom_line(col = "darkgreen") +
  geom_point(col = "darkgreen") + 
  labs(x = "Area", y = "Predicted Price", title = "As area increases HousePrice increases", subtitle = "Partial Dependency plot") +
  theme_mi2()
```


As we move from South Mumbai to North Mumbai, House price declines. 
Note: 18.9 is south and 19.2 is North
```{r}
as.tibble(b) %>%  
  ggplot(aes(x, y)) + 
  geom_line(col = "darkgreen") +
  geom_point(col = "darkgreen") + 
  geom_text(x = 18.98, y = 25000, label = "South Mumbai", col = "black", family = "mono") +
  geom_text(x = 19.21, y = 15500, label = "North Mumbai", col = "black", family = "mono") +
  labs(x = "Lat", y = "Predicted Price", title = "As we move from South Mumbai to North Mumbai, House price declines", subtitle = "Partial Dependency plot") +
  theme_mi2()
```

As we move from west to east, House price declines.
Not 72.80 is East and 72.95 is west - plot on graph.

```{r, eval=FALSE, echo=FALSE, include=FALSE}
library(grid)
West_Mumbai <- textGrob("West Mumbai") #Enter extra formating after a comma

p <- as.tibble(c) %>%  
  ggplot(aes(x, y)) + 
  geom_line(col = "darkgreen") +
  geom_point(col = "darkgreen") + 
  labs(x = "Lat", y = "y_hat", 
       title = "As we move from West Mumbai to East Mumbai, House price declines", 
       subtitle = "Partial Dependency plot") + 
  theme_bw() + 
  annotation_custom(West_Mumbai, xmin=72.80, xmax=72.85, ymin=12500, ymax=13000)

gt <- ggplot_gtable(ggplot_build(p))
gt$layout$clip[gt$layout$name == "panel"] <- "off"

grid.draw(gt)
```

```{r}
as.tibble(c) %>%  
  ggplot(aes(x, y)) + 
  geom_line(col = "darkgreen") +
  geom_point(col = "darkgreen") + 
  geom_text(x = 72.80, y = 22500, label = "West Mumbai", col = "black", family = "mono") +
  geom_text(x = 72.95, y = 14200, label = "East Mumbai", col = "black", family = "mono") +
  labs(x = "Lat", y = "Predicted Price", title = "As we move from West Mumbai to South Mumbai, House price declines", subtitle = "Partial Dependency plot") + 
  theme_mi2()
```


```{r}
single_variable(xgb_explainer, "Area") %>% plot()
```







