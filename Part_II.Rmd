---
title: "Data Cleaning, Quality Enhancing and Feature Enginering"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

In this section we will prepare our data for machine learning models. We will perform feature enginering, quality enhancing and Data profiling.

This section is an essential and important part in machine learning process. Here is how Max Kuhn descrbe its importance.

> At face value, model building appears straightforward: pick a modeling technique, plug in data, and generate a prediction. While this approach will generate a predictive model, it will most likely not generate a reliable, trustworthy model for predicting new samples. To get this type of model, we must first understand the data and the objective of the modeling -  Applied Machine Learning

## Read data

The scrapped data can be downloaded [from here](). 

```{r, load_library, read_data, message=FALSE, warning=FALSE}

library(plotly); library(DT); library(broom); library(caret)
library(tidyverse); 

nb_data_csv <- read_csv("nb_data.csv")

```

## Feature Enginering

Some varaibles (columns) in data set are repetative or words such as Address. We remove these variables as they will not be helpful to our model. 

There are only 46 (0.68%) entries for 4+ Bedrooms, since we cannot clearly identify the number of Bedrooms we remove them.  
Note: Since they represent a small fraction of our data, they can be safely removed.

```{r, load_data}

nb_data <- nb_data_csv %>% 
  select(-Measure, -Verified, -LoanVerified, 
         -SaleType, -Address, -PostedTime, 
         -ShortAddress, -Area2, -MonthlyEMI, 
         -propid, -HouseType, -FloorType2) %>% 
  filter(Bedrooms != "4+ bedrooms")
  
```

The house price, bedrooms, bathroom and balconies are a combination of figures and words.  
We convert them to proper figures.

The owned by varaible has low cardinality, we reduce the categories into two groups.

```{r, step_clean}

nb_data <- nb_data %>% 
  separate(Price, c("Amt", "word"), sep = " ") %>% 
  mutate(wordtonum = 
           case_when(
             word == "Crores" ~ 10000000,
             word == "Crore" ~ 10000000,
             word == "Lacs" ~ 100000,
             word == "K" ~ 1000
           )) %>% 
  mutate(HousePrice = as.numeric(Amt) * wordtonum) %>% 
  separate(Bedrooms , c("NoofBedrooms", "Delete1"), sep = " ") %>% 
  separate(Bathrooms , c("NoofBathrooms", "Delete2"), sep = " ") %>% 
  separate(Balconies, c("NoofBalconies", "Delete3"), sep = " ") %>%
  separate(Maintanance, c("Maint_sqft", "Delete4", "Delete5"), sep = " ") %>% 
  separate(FloorOn, c("Delete6", "OnFloor", "Delete7", "NooffloorsinBldg"), sep = " ") %>% 
  select(-Amt, -word, -starts_with("Delete"), - wordtonum) %>% 
  mutate_at(c("NoofBedrooms", "NoofBathrooms", "NoofBalconies", "Maint_sqft", "OnFloor", "NooffloorsinBldg"), 
            as.numeric) %>% 
  mutate(OwnedBy = ifelse(str_detect(OwnedBy, "On Lease"), "Rented", "Owned"))
  

```

Next, we scale house price on '000 scale and convert categorical variables into factors.

```{r, step_feature_eng}
nb_data <- nb_data %>% 
  mutate(HousePriceinK = HousePrice/ 1000) %>% 
  select(-HousePrice) %>% 
  mutate_at(c("Negotiable", "FloorType", "Bldgtype", "Age", "OwnedBy", "Furnished", "Facing", "Parking", "PowerBackup", "WaterSupply"), as.factor)
```

The Latitude and Longitude can help us to extract zip code using `ggmap` package. The zip code provides us options to identify new variables such as number of schools, hospitals, fire bridage etc. 

Extracting such information is a time consuming task, which can be performed on some other day.

## Enhance Data Quality

### Data Summary

```{r, show_summary_1}
summary(nb_data)
```

From data summary, a few points looks suspicious. A maximum area of 12,83,800 and a monthly maintanance of 18,888 per sq ft!

### Outilers

We identified few outlier from summary, next we have a graphical view to identify outliers.

```{r, outlier_1}
nb_data %>% 
  plot_ly(x = ~Age, y = ~Area) %>% 
  add_boxplot()
  
```

From the above visualization, we confirm that there is an outlier in the data. The House Area of the outlier is 12,83,800. Lets remove it and redraw the plot.

```{r, outlier_2}
nb_data <- nb_data %>% 
  filter(Area < 10000) 

nb_data%>% 
  plot_ly(x = ~Age, y = ~Area) %>% 
  add_boxplot()
  
```

There is still an outlier on the chart. Lets have a close look at it.

```{r, oultier_3}
nb_data %>% 
  select(Area, NoofBedrooms, NoofBathrooms, NooffloorsinBldg) %>% 
  arrange(desc(Area)) %>% 
  slice(1) %>%
  t() %>% as.data.frame() %>% 
  rownames_to_column() %>% as.tibble() %>% 
  rename("Details of Outlier" = "rowname", "Values" = "V1") %>% 
  datatable(class = 'cell-border stripe', rownames = FALSE, options = list(dom = "t"))

```

Ah! A 14 stored bldg having an area of 9000 with 2 Bedrooms, though it is possible but it seems suspisious. For now we will remove this entry as well.

```{r, oultier_4}
nb_data <- nb_data %>% 
  filter(Area < 8999)
```

```{r, oultier_5}
nb_data %>% 
  plot_ly(x = ~log(HousePriceinK)) %>% 
  add_histogram()
```

We have a few more outliers, they are not going away easily, remember 80% of your time!

```{r, oultier_6}

nb_data <- nb_data %>% 
  filter(HousePriceinK > 100)

age_order <- c("Under Construction", "Newly Constructed", "1-3 years old", "3-5 years old", "5-10 years old")

nb_data %>% 
  ggplot(aes(log(HousePriceinK))) + 
  geom_histogram(bins = 100) + 
  facet_grid(. ~ fct_relevel(Age, age_order)) + 
  labs(x = "(Log) House Price", y = "Histogram count") 
  
```

Using a similar approach, we identify and remove outliers for Maintainance.

```{r, oultier_7}
nb_data <- nb_data %>% 
  filter(Maint_sqft < 50)
```


```{r, show_summary_2}
nb_data %>% summary()
```

### Missing Values

By using summary we see that

```{r, missing_data}
nb_data %>% 
  summary() %>% 
  tidy() %>% 
  filter(str_detect(Freq, "NA's")) %>% 
  select("Variable" = Var2, "NACount" = Freq)
```

We will keep this information handy for now and look at various options for imputing in our machine learning process.

Some of the Impute options that we may use are:

 1) Impute as Unknown - since these are categorical variables, we can directly use them as unknown.  
 2) K Nearast Neighbour Impute - We can use a distance technqiue to identify neareast neighbour and impute accordingly.  
 3) Bagged impute - We can take various sample from our dataset and deicide imputing value based on majority.  
 
```{r, save_nb_data}
saveRDS(nb_data, "nb_data.rds")
```

## Data Profiling

### Correlation

Not surprising, Area is highly correlated with House Price.
As we move towards north from south, price starts declining.
similarly for east to west!

```{r, corr_1}
nb_data %>% select_if(is.numeric) %>% stats::cor() %>% corrplot()
```

### Near Zero Variance

If a variable has low cardinality or frequency, they may breakdown your model e.g. they may be picked up in train set and skip by test set.

The concept of near zero variance is best understood through an example. In a conference all members belong to bottom level of managment except a few guest. The near zero variance helps us identify numeric categories that are of lower order.

On the numeric variables we see the **nzv** value as false and hence they are fine!

```{r, nzv_1}
nb_data %>% 
  select_if(is.numeric) %>% 
  nearZeroVar(saveMetrics = TRUE)
```

On categories we notie **nzv** is true for OwnedBy, giving us an indication that the categories are not spread properly.

The BldgType also has a high freqRatio, we verify the output and remove this predictor so that our model do not break because of low counts of data.

```{r, nza_2}
nb_data %>% 
  select_if(is.factor) %>% 
  nearZeroVar(saveMetrics = TRUE)
```


```{r, nzv_3}
#use arrnage to add grobs
nb_data <- nb_data %>% 
  select(-Bldgtype, -OwnedBy)
```


### Find Linear combinations

When a dataset has large number of varaible, there are chances that few variables are combination of two or more varaibles.

Example: Total house area is a combination of bedrooms, bathrooms etc, the `findLinearCombos` identify such varaible that are a combination of two are more varaibles within the dataset.

```{r, lin_comb_1}
nb_data %>% 
  select_if(is.numeric) %>% 
  findLinearCombos()
```

```{r, lin_comb_2}
nb_data %>% 
  summary()
```

### Normal Distribution

```{r}

```


Now that we are ready with our data lets proceed to build models.

