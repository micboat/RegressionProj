---
title: "Data Cleaning, Quality Enhancing and Feature Enginering"
output: 
  html_document:
    code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache=TRUE)
```

## Dataset

In step one of our project, we extracted data from NoBroker.in. In this step we will prepare our data for machine learning models, i.e we will be looking at data cleaning, quality checking and feature enginering.


> At face value, model building appears straightforward: pick a modeling technique, plug in data, and generate a prediction. While this approach will generate a predictive model, it will most likely not generate a reliable, trustworthy model for predicting new samples. To get this type of model, we must first understand the data and the objective of the modeling.

> --Max Kuhn, Kjell Johnson -  Applied Machine Learning

## Read data

The scrapped data can be downloaded [from here](). 

```{r read_data, message=FALSE, warning=FALSE}

library(tidyverse); library(plotly); library(DT); library(broom); library(caret)

nb_data_csv <- read_csv("nb_data.csv")

```

## Clean Data

First let us remove unwanted variables from the dataset. These variables are either redundunt such as in LoanVerified or character values such as Address. 

There are only 46 entries for 4+ Bedrooms, since we cannot clearly identify the number of Bedrooms for these entries and they represent only 0.68% of the total data, these can be safely removed.

```{r}

nb_data <- nb_data_csv %>% 
  select(-Measure, -Verified, -LoanVerified, -SaleType, -Address, -PostedTime, -ShortAddress, -Area2, -MonthlyEMI, -propid, -HouseType, -FloorType2) %>% 
  filter(Bedrooms != "4+ bedrooms")
  
```

Convert character columns Price, Bedrooms, Bathrooms etc into numeric columns


```{r}

nb_data <- nb_data %>% 
  separate(Price, c("Amt", "word"), sep = " ") %>% 
  mutate(wordtonum = 
           case_when(
             word == "Crores" ~ 10000000,
             word == "Crore" ~ 10000000,
             word == "Lacs" ~ 100000,
             word == "K" ~ 1000
           )) %>% 
  mutate(HousePrice = as.numeric(Amt) * wordtonum) %>% 
  separate(Bedrooms , c("NoofBedrooms", "Delete1"), sep = " ") %>% 
  separate(Bathrooms , c("NoofBathrooms", "Delete2"), sep = " ") %>% 
  separate(Balconies, c("NoofBalconies", "Delete3"), sep = " ") %>%
  separate(Maintanance, c("Maint_sqft", "Delete4", "Delete5"), sep = " ") %>% 
  separate(FloorOn, c("Delete6", "OnFloor", "Delete7", "NooffloorsinBldg"), sep = " ") %>% 
  select(-Amt, -word, -starts_with("Delete"), - wordtonum) %>% 
  mutate_at(c("NoofBedrooms", "NoofBathrooms", "NoofBalconies", "Maint_sqft", "OnFloor", "NooffloorsinBldg"), 
            as.numeric) %>% 
  mutate(OwnedBy = ifelse(str_detect(OwnedBy, "On Lease"), "Rented", "Owned"))
  

```

## Feature Enginerring

Lets scale price on '000 scale.

```{r}
nb_data <- nb_data %>% 
  mutate(HousePriceinK = HousePrice/ 1000) %>% 
 # mutate(Maint_sqft = Maint_sqft * Area) %>% 
  select(-HousePrice) %>% 
  mutate_at(c("Negotiable", "FloorType", "Bldgtype", "Age", "OwnedBy", "Furnished", "Facing", "Parking", "PowerBackup", "WaterSupply"), as.factor) #%>% 
  # mutate(NoofBedrooms = factor(NoofBedrooms, levels = 0:4, ordered = TRUE)) %>% 
  # mutate(NoofBathrooms = factor(NoofBathrooms, levels = 0:8, ordered = TRUE)) %>% 
  # mutate(NoofBalconies = factor(NoofBalconies, levels = 0:11, ordered = TRUE)) %>% 
  # mutate(NooffloorsinBldg = factor(NooffloorsinBldg, levels = 0:117, ordered = TRUE)) %>% 
  # mutate(OnFloor = factor(OnFloor, levels = 0:56, ordered = TRUE))
```

```{r}
summary(nb_data)
```


## Outilers

From above summary, there are some points that looks suspicious. 
TODO Specify why Age

Maximum Area = 1283800 Well thats huge
Maintanance = 18888 Again Huge

```{r}
nb_data %>% 
  plot_ly(x = ~Age, y = ~Area) %>% 
  add_boxplot()
  
```

From the visualization, we can confirm that there is an outlier in the data. The House Area of the outlier is 12,83,800, well that almost one third of India. Lets remove it and redraw the plot.

```{r}
nb_data <- nb_data %>% 
  filter(Area < 10000) 

nb_data%>% 
  plot_ly(x = ~Age, y = ~Area) %>% 
  add_boxplot()
  
```

There is still an outlier on the chart. Lets have a close look at it.

```{r}
nb_data %>% 
  select(Area, NoofBedrooms, NoofBathrooms, NooffloorsinBldg) %>% 
  arrange(desc(Area)) %>% 
  slice(1) %>%
  t() %>% as.data.frame() %>% 
  rownames_to_column() %>% as.tibble() %>% 
  rename("Details of Outlier" = "rowname", "Values" = "V1") %>% 
  datatable(class = 'cell-border stripe', rownames = FALSE, options = list(dom = "t"))

```

Ah! A 14 stored bldg having an area of 9000 with 2 Bedrooms, though it is possible but it seems suspisious. Since we have enough data (considering this project is at a company where data flow is adequate), we will remove this entry as well.

In a similar fashion, we remove maintainance outliers

```{r}
nb_data <- nb_data %>% 
  filter(Area < 8999)
```

```{r}
nb_data %>% 
  plot_ly(x = ~log(HousePriceinK)) %>% 
  add_histogram()
```

We have a few more outliers, they are not going away easily, remember 80% of your time!

```{r}

nb_data <- nb_data %>% 
  filter(HousePriceinK > 100)

age_order <- c("Under Construction", "Newly Constructed", "1-3 years old", "3-5 years old", "5-10 years old")

nb_data %>% 
  ggplot(aes(log(HousePriceinK))) + 
  geom_histogram(bins = 100) + 
  facet_grid(. ~ fct_relevel(Age, age_order)) + 
  labs(x = "(Log) House Price", y = "Histogram count") 
  
```

In a similar fashion we remove outliers for Maintainance

```{r}
nb_data <- nb_data %>% 
  filter(Maint_sqft < 50)
```


```{r}
nb_data %>% summary()
```

## Missing Values

By using summary we see that

```{r}
nb_data %>% 
  summary() %>% 
  tidy() %>% 
  filter(str_detect(Freq, "NA's")) %>% 
  select("Variable" = Var2, "NACount" = Freq) #%>% 
  #knitr::kable()
  #datatable(class = 'cell-border stripe', options = list(dom = "t"), rownames = FALSE)
```



We will keep this information handy for now and look at various options for imputing in our machine learning process.

Some of the Impute options that we will be testing are:

 1) Impute as Unkown - since this are categorical variables, we can directly use them as unknown
 2) K Nearast Neighbour Impute - We can use various distance technqiue to identify neareast neighbour and impute accordingly
 3) Bagged impute - We will take various sample from our dataset and deicide imputing value based on majority or average in case of numeric.
 
If you with me so far, lets move to the to Part III of this Machine Learning process - the tidy approach.

## Near Zero Variance

```{r}
nb_data %>% 
  select_if(is.numeric) %>% 
  nearZeroVar(saveMetrics = TRUE)
```

```{r}
nb_data %>% 
  select_if(is.factor) %>% 
  nearZeroVar(saveMetrics = TRUE)
```

OwnedBy - we can see near zero variance (nzv column) return TRUE, since this has more repeated column we will remove this.

BldgType also has a high freqRatio, we verified the output and decided to remove it so that our models do not break

```{r}
nb_data <- nb_data %>% 
  select(-Bldgtype, -OwnedBy)
```


## Find Linear combinations

```{r}
nb_data %>% 
  select_if(is.numeric) %>% 
  findLinearCombos()
```

```{r}
nb_data %>% 
  summary()
```

## Correlation

```{r}
nb_data %>% select_if(is.numeric) %>% stats::cor() %>% corrplot()
```

Not surprising, Area is highly correlated with House Price.
As we move towards north from south, price starts declining.
similarly for east to west!

```{r}
saveRDS(nb_data, "nb_data.rds")
```


## Outcomes
Impute Missing Value
No of Bathrooms, No of Balconies <- use step_other

check Minimum price on House, Remove Module 3.

